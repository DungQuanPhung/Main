{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ea481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NeuralNetworkXOR:\n",
    "    def __init__(self, x_mau, y_mau, num_input_features, num_hidden_nodes, num_output_nodes, num_layers, max_iterations, lr):\n",
    "        self.x_mau = x_mau\n",
    "        self.y_mau = y_mau\n",
    "        self.num_input_features = num_input_features\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        self.num_output_nodes = num_output_nodes\n",
    "        self.num_layers = num_layers\n",
    "        self.max_iterations = max_iterations\n",
    "        self.lr = lr\n",
    "        self.w = [None] * (self.num_layers + 1)\n",
    "        self.n = [None] * (self.num_layers + 1)\n",
    "        self.H = [None] * (self.num_layers + 1)\n",
    "        self.layer_inputs_with_bias = [None] * (self.num_layers + 1)\n",
    "        self.delta = [None] * (self.num_layers + 1)\n",
    "        self.dw = [None] * (self.num_layers + 1)        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, n):\n",
    "        s = self.sigmoid(n)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        np.random.seed(0)\n",
    "        # Lớp 1 (lớp ẩn đầu tiên hoặc lớp output nếu num_layers=1)\n",
    "        if self.num_layers == 1:\n",
    "            self.w[1] = np.random.uniform(-1, 1, (self.num_output_nodes, self.num_input_features + 1))\n",
    "        else:\n",
    "            self.w[1] = np.random.uniform(-1, 1, (self.num_hidden_nodes, self.num_input_features + 1))\n",
    "        for i in range(2, self.num_layers + 1):\n",
    "            if i == self.num_layers:  # Lớp output\n",
    "                self.w[i] = np.random.uniform(-1, 1, (self.num_output_nodes, self.num_hidden_nodes + 1))\n",
    "            else:  # Lớp ẩn khác\n",
    "                self.w[i] = np.random.uniform(-1, 1, (self.num_hidden_nodes, self.num_hidden_nodes + 1))\n",
    "\n",
    "    def _forward_pass(self, x_in, N_samples):\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            if i == 1:\n",
    "                current_layer_input_actual = x_in\n",
    "            else:\n",
    "                prev_layer_H_output = self.H[i - 1]\n",
    "                bias_row = np.ones((1, N_samples))\n",
    "                current_layer_input_actual = np.vstack((bias_row, prev_layer_H_output))           \n",
    "            self.layer_inputs_with_bias[i] = current_layer_input_actual          \n",
    "            self.n[i] = np.dot(self.w[i], current_layer_input_actual)\n",
    "            self.H[i] = self.sigmoid(self.n[i])       \n",
    "        return self.H[self.num_layers]\n",
    "\n",
    "    def _compute_loss(self, predict, y_true):\n",
    "        epsilon = 1e-8\n",
    "        J_cost = -(y_true * np.log(predict + epsilon) + (1 - y_true) * np.log(1 - predict + epsilon))\n",
    "        return np.mean(J_cost)\n",
    "\n",
    "    def _backpropagation(self, y_true, N_samples):\n",
    "        for i in range(self.num_layers, 0, -1):\n",
    "            if i == self.num_layers:\n",
    "                self.delta[i] = (self.H[i] - y_true) * self.sigmoid_derivative(self.n[i])\n",
    "            else:\n",
    "                weights_from_next_layer_no_bias = self.w[i + 1][:, 1:]\n",
    "                error_propagated_sum = np.dot(weights_from_next_layer_no_bias.T, self.delta[i + 1])\n",
    "                self.delta[i] = error_propagated_sum * self.sigmoid_derivative(self.n[i])           \n",
    "            self.dw[i] = np.dot(self.delta[i], self.layer_inputs_with_bias[i].T) / N_samples\n",
    "\n",
    "    def _update_weights(self):\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            self.w[i] -= self.lr * self.dw[i]\n",
    "\n",
    "    def train(self):\n",
    "        N = self.x_mau.shape[0]\n",
    "        ones_column = np.ones((N, 1))\n",
    "        x_in = np.hstack((ones_column, self.x_mau)).T\n",
    "        y_true = self.y_mau.T\n",
    "        loss_history = []\n",
    "        for epoch in range(self.max_iterations):\n",
    "            # forward pass\n",
    "            predict = self._forward_pass(x_in, N)          \n",
    "            # Compute loss\n",
    "            current_loss = self._compute_loss(predict, y_true)\n",
    "            loss_history.append(current_loss)\n",
    "            if epoch % 1000 == 0 or epoch == self.max_iterations - 1:\n",
    "                print(f\"Epoch {epoch}, Loss: {current_loss:.4f}\")\n",
    "            # Backpropagation\n",
    "            self._backpropagation(y_true, N)           \n",
    "            # Update weights\n",
    "            self._update_weights()    \n",
    "        return loss_history\n",
    "\n",
    "    def predict(self):\n",
    "        N = self.x_mau.shape[0]\n",
    "        ones_column = np.ones((N, 1))\n",
    "        x_in = np.hstack((ones_column, self.x_mau)).T\n",
    "        # Chỉ cần lan truyền xuôi để dự đoán\n",
    "        predict = self._forward_pass(x_in, N).T \n",
    "        final_predictions = np.round(predict)      \n",
    "        return predict, final_predictions\n",
    "    \n",
    "x_mau = np.array([\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1]\n",
    "])\n",
    "y_mau = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "num_samples = x_mau.shape[0]\n",
    "num_input_features = x_mau.shape[1]\n",
    "num_output_nodes = y_mau.shape[1]\n",
    "\n",
    "nn_xor = NeuralNetworkXOR(\n",
    "    x_mau,\n",
    "    y_mau,\n",
    "    num_input_features=num_input_features,\n",
    "    num_hidden_nodes=3, # Số nơ-ron ẩn\n",
    "    num_output_nodes=num_output_nodes,\n",
    "    num_layers=2,   # Số tầng ẩn\n",
    "    max_iterations=10000,\n",
    "    lr=0.1\n",
    ")\n",
    "loss_history = nn_xor.train()\n",
    "output_probs, predictions = nn_xor.predict()\n",
    "for i in range(x_mau.shape[0]):\n",
    "    print(f\"Input: {x_mau[i]}, Expected: {y_mau[i][0]}, Predicted: {int(predictions[i][0])}, Xác Suất: {output_probs[i][0]:.4f}\")\n",
    "\n",
    "# Vẽ đồ thị loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history, label='Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
