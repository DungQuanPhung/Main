{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7RKVKPHwrCy",
        "outputId": "fbce126e-b6ef-4e61-c757-b5cdd125e8fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---- ----------------------------------- 1.3/12.8 MB 7.5 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 5.5/12.8 MB 14.6 MB/s eta 0:00:01\n",
            "     ----------------------------------- --- 11.8/12.8 MB 20.0 MB/s eta 0:00:01\n",
            "     --------------------------------------- 12.8/12.8 MB 19.6 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "R6D6QbVBjuCf"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re\n",
        "from time import time\n",
        "import spacy\n",
        "import multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RDKjOSAllZa",
        "outputId": "2b989dc2-7858-4ba5-8e80-ecc1c9b31161"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\n",
        "!wget https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'news_summary (2).csv'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wget\n",
        "wget.download('https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv')\n",
        "wget.download('https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OFMgrWn8mruc"
      },
      "outputs": [],
      "source": [
        "summary = pd.read_csv('news_summary.csv', encoding='iso-8859-1', sep=',')\n",
        "raw = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1', sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "xR6sB0SvokIp",
        "outputId": "b7d30dd4-5619-4842-cffc-9d795673f03c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "author                                            Chhavi Tyagi\n",
              "date                                      03 Aug 2017,Thursday\n",
              "headlines    Daman & Diu revokes mandatory Rakshabandhan in...\n",
              "read_more    http://www.hindustantimes.com/india-news/raksh...\n",
              "text         The Administration of Union Territory Daman an...\n",
              "ctext        The Daman and Diu administration on Wednesday ...\n",
              "Name: 0, dtype: object"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "WBnnUYw4ovBk",
        "outputId": "be062d96-78c0-4c40-f57e-370c7784ca88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "headlines    upGrad learner switches to career in ML & Al w...\n",
              "text         Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
              "Name: 0, dtype: object"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ICuz2G3TnIG4"
      },
      "outputs": [],
      "source": [
        "def process_news_data(raw, summary):\n",
        "    \"\"\"\n",
        "    Hàm xử lý hai tập dữ liệu news và trả về DataFrame với cột 'text' và 'summary'.\n",
        "\n",
        "    Parameters:\n",
        "    raw (pd.DataFrame): DataFrame chứa cột 'headlines' và 'text'.\n",
        "    summary (pd.DataFrame): DataFrame chứa cột 'author', 'date', 'read_more', 'text', 'ctext', 'headlines'.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame chứa cột 'text' và 'summary'.\n",
        "    \"\"\"\n",
        "    # Tạo bản sao của raw với cột 0 và 1 (headlines, text)\n",
        "    pre1 = raw.iloc[:, 0:2].copy()\n",
        "\n",
        "    # Tạo bản sao của summary với cột 0 đến 5 (author, date, read_more, text, ctext, headlines)\n",
        "    pre2 = summary.iloc[:, 0:6].copy()\n",
        "\n",
        "    # Kết hợp các cột trong pre2 thành cột 'text'\n",
        "    pre2['text'] = pre2['author'].str.cat(\n",
        "        pre2['date'].str.cat(\n",
        "            pre2['read_more'].str.cat(\n",
        "                pre2['text'].str.cat(pre2['ctext'], sep=\" \"),\n",
        "                sep=\" \"\n",
        "            ),\n",
        "            sep=\" \"\n",
        "        ),\n",
        "        sep=\" \"\n",
        "    )\n",
        "\n",
        "    # Tạo DataFrame pre và nối các cột từ pre1 và pre2\n",
        "    pre = pd.DataFrame()\n",
        "    pre['text'] = pd.concat([pre1['text'], pre2['text']], ignore_index=True)\n",
        "    pre['summary'] = pd.concat([pre1['headlines'], pre2['headlines']], ignore_index=True)\n",
        "\n",
        "    return pre\n",
        "\n",
        "#Removes non-alphabetic characters:\n",
        "def text_strip(column):\n",
        "    for row in column:\n",
        "\n",
        "        #ORDER OF REGEX IS VERY VERY IMPORTANT!!!!!!\n",
        "\n",
        "        row=re.sub(\"(\\\\t)\", ' ', str(row)).lower() #remove escape charecters\n",
        "        row=re.sub(\"(\\\\r)\", ' ', str(row)).lower()\n",
        "        row=re.sub(\"(\\\\n)\", ' ', str(row)).lower()\n",
        "\n",
        "        row=re.sub(\"(__+)\", ' ', str(row)).lower()   #remove _ if it occors more than one time consecutively\n",
        "        row=re.sub(\"(--+)\", ' ', str(row)).lower()   #remove - if it occors more than one time consecutively\n",
        "        row=re.sub(\"(~~+)\", ' ', str(row)).lower()   #remove ~ if it occors more than one time consecutively\n",
        "        row=re.sub(\"(\\+\\++)\", ' ', str(row)).lower()   #remove + if it occors more than one time consecutively\n",
        "        row=re.sub(\"(\\.\\.+)\", ' ', str(row)).lower()   #remove . if it occors more than one time consecutively\n",
        "\n",
        "        row=re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(row)).lower() #remove <>()|&©ø\"',;?~*!\n",
        "\n",
        "        row=re.sub(\"(mailto:)\", ' ', str(row)).lower() #remove mailto:\n",
        "        row=re.sub(r\"(\\\\x9\\d)\", ' ', str(row)).lower() #remove \\x9* in text\n",
        "        row=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(row)).lower() #replace INC nums to INC_NUM\n",
        "        row=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(row)).lower() #replace CM# and CHG# to CM_NUM\n",
        "\n",
        "\n",
        "        row=re.sub(\"(\\.\\s+)\", ' ', str(row)).lower() #remove full stop at end of words(not between)\n",
        "        row=re.sub(\"(\\-\\s+)\", ' ', str(row)).lower() #remove - at end of words(not between)\n",
        "        row=re.sub(\"(\\:\\s+)\", ' ', str(row)).lower() #remove : at end of words(not between)\n",
        "\n",
        "        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n",
        "\n",
        "        #Replace any url as such https://abc.xyz.net/browse/sdf-5327 ====> abc.xyz.net\n",
        "        try:\n",
        "            url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(row))\n",
        "            repl_url = url.group(3)\n",
        "            row = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)',repl_url, str(row))\n",
        "        except:\n",
        "            pass #there might be emails with no url in them\n",
        "\n",
        "\n",
        "\n",
        "        row = re.sub(\"(\\s+)\",' ',str(row)).lower() #remove multiple spaces\n",
        "\n",
        "        #Should always be last\n",
        "        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n",
        "\n",
        "\n",
        "\n",
        "        yield row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BiK5fivEvgdU"
      },
      "outputs": [],
      "source": [
        "pre = process_news_data(raw, summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UcM-TalNvmXN",
        "outputId": "c7359991-da37-4797-f090-7b78ed9f2010"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "text",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "summary",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "4a0638d1-811d-4242-97c2-4e16458d2302",
              "rows": [
                [
                  "0",
                  "Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.",
                  "upGrad learner switches to career in ML & Al with 90% salary hike"
                ],
                [
                  "1",
                  "Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.",
                  "Delhi techie wins free food from Swiggy for one year on CRED"
                ],
                [
                  "2",
                  "New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma's captaincy after 12 consecutive victories dating back to March 2018. The match witnessed India getting all out for 92, their seventh lowest total in ODI cricket history.",
                  "New Zealand end Rohit Sharma-led India's 12-match winning streak"
                ],
                [
                  "3",
                  "With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to Ã¢ÂÂ¹46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, customers have options to insure against Critical Illnesses, Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years.",
                  "Aegon life iTerm insurance plan helps customers save tax"
                ],
                [
                  "4",
                  "Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"In the #MeToo movement, I always believe a woman. But in this case, we need to reserve our judgment,\" she added. Hirani has been accused by an assistant who worked in 'Sanju'.",
                  "Have known Hirani for yrs, what if MeToo claims are not true: Sonam"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Speaking about the sexual harassment allegatio...</td>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...   \n",
              "1  Kunal Shah's credit card bill payment platform...   \n",
              "2  New Zealand defeated India by 8 wickets in the...   \n",
              "3  With Aegon Life iTerm Insurance plan, customer...   \n",
              "4  Speaking about the sexual harassment allegatio...   \n",
              "\n",
              "                                             summary  \n",
              "0  upGrad learner switches to career in ML & Al w...  \n",
              "1  Delhi techie wins free food from Swiggy for on...  \n",
              "2  New Zealand end Rohit Sharma-led India's 12-ma...  \n",
              "3  Aegon life iTerm insurance plan helps customer...  \n",
              "4  Have known Hirani for yrs, what if MeToo claim...  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "collapsed": true,
        "id": "FzU6bNSXvwVM",
        "outputId": "d23850bf-4ed0-4c4b-a64b-4fd9726e6a10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
              "1    Kunal Shah's credit card bill payment platform...\n",
              "2    New Zealand defeated India by 8 wickets in the...\n",
              "3    With Aegon Life iTerm Insurance plan, customer...\n",
              "4    Speaking about the sexual harassment allegatio...\n",
              "5    Pakistani singer Rahat Fateh Ali Khan has deni...\n",
              "6    India recorded their lowest ODI total in New Z...\n",
              "7    Weeks after ex-CBI Director Alok Verma told th...\n",
              "8    Andhra Pradesh CM N Chandrababu Naidu has said...\n",
              "9    Congress candidate Shafia Zubair won the Ramga...\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre['text'][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "y55PRoIiwRjs"
      },
      "outputs": [],
      "source": [
        "brief_cleaning1 = text_strip(pre['text'])\n",
        "brief_cleaning2 = text_strip(pre['summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujlq55elwcrP",
        "outputId": "c55fd3a3-9c76-40b8-9ddf-c65a24666918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to clean up everything: 0.99 mins\n"
          ]
        }
      ],
      "source": [
        "# Load the model with the full name and disable components for speed\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
        "\n",
        "t = time()\n",
        "\n",
        "# Batch the data points into 5000 and run on all cores for faster preprocessing\n",
        "num_processes = max(1, multiprocessing.cpu_count() // 2)\n",
        "text = [str(doc) for doc in nlp.pipe(brief_cleaning1, batch_size=1000, n_process=-1)]  # Changed n_threads to n_process\n",
        "\n",
        "# Print processing time\n",
        "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-sL1ELmyjHI",
        "outputId": "103a7328-814f-42cc-b64a-4bff6e7c8e8a"
      },
      "outputs": [],
      "source": [
        "#Taking advantage of spaCy .pipe() method to speed-up the cleaning process:\n",
        "\n",
        "t = time()\n",
        "\n",
        "#Batch the data points into 5000 and run on all cores for faster preprocessing\n",
        "summary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(brief_cleaning2, batch_size=1000, n_process=-1)]\n",
        "\n",
        "#Takes 7-8 mins\n",
        "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi-s97I7Kapd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_word_count_histogram(text, summary):\n",
        "    \"\"\"\n",
        "    Create a histogram comparing word counts of text and summary.\n",
        "\n",
        "    Parameters:\n",
        "    text (list): List of text strings\n",
        "    summary (list): List of summary strings\n",
        "\n",
        "    Returns:\n",
        "    None (displays histogram)\n",
        "    \"\"\"\n",
        "    # Create DataFrame with text and summary\n",
        "    pre = pd.DataFrame({\n",
        "        'cleaned_text': pd.Series(text),\n",
        "        'cleaned_summary': pd.Series(summary)\n",
        "    })\n",
        "\n",
        "    # Calculate word counts\n",
        "    text_count = [len(sent.split()) for sent in pre['cleaned_text']]\n",
        "    summary_count = [len(sent.split()) for sent in pre['cleaned_summary']]\n",
        "\n",
        "    # Create DataFrame for graphing\n",
        "    graph_df = pd.DataFrame({\n",
        "        'text': text_count,\n",
        "        'summary': summary_count\n",
        "    })\n",
        "\n",
        "    # Plot histogram\n",
        "    graph_df.hist(bins=5)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "8NYu-IaMKdpr",
        "outputId": "eb8d22eb-23e5-4943-daeb-3edc7778f4d4"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'split'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcreate_word_count_histogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mcreate_word_count_histogram\u001b[39m\u001b[34m(text, summary)\u001b[39m\n\u001b[32m     15\u001b[39m pre = pd.DataFrame({\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m: pd.Series(text),\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcleaned_summary\u001b[39m\u001b[33m'\u001b[39m: pd.Series(summary)\n\u001b[32m     18\u001b[39m })\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Calculate word counts\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m text_count = \u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcleaned_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     22\u001b[39m summary_count = [\u001b[38;5;28mlen\u001b[39m(sent.split()) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m pre[\u001b[33m'\u001b[39m\u001b[33mcleaned_summary\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Create DataFrame for graphing\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     15\u001b[39m pre = pd.DataFrame({\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m: pd.Series(text),\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcleaned_summary\u001b[39m\u001b[33m'\u001b[39m: pd.Series(summary)\n\u001b[32m     18\u001b[39m })\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Calculate word counts\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m text_count = [\u001b[38;5;28mlen\u001b[39m(\u001b[43msent\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m()) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m pre[\u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     22\u001b[39m summary_count = [\u001b[38;5;28mlen\u001b[39m(sent.split()) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m pre[\u001b[33m'\u001b[39m\u001b[33mcleaned_summary\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Create DataFrame for graphing\u001b[39;00m\n",
            "\u001b[31mAttributeError\u001b[39m: 'float' object has no attribute 'split'"
          ]
        }
      ],
      "source": [
        "create_word_count_histogram(text, summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOnpXbvkOUx6"
      },
      "outputs": [],
      "source": [
        "pre['cleaned_text'] = pd.Series(text)\n",
        "pre['cleaned_summary'] = pd.Series(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhAjrs7fOADB"
      },
      "outputs": [],
      "source": [
        "#Model to summarize the text between 0-15 words for Summary and 0-100 words for Text\n",
        "max_text_len=100\n",
        "max_summary_len=15\n",
        "cleaned_text =np.array(pre['cleaned_text'])\n",
        "cleaned_summary=np.array(pre['cleaned_summary'])\n",
        "\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "\n",
        "post_pre=pd.DataFrame({'text':short_text,'summary':short_summary})\n",
        "post_pre['summary'] = post_pre['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WELAbpiuN4hF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(post_pre['text']),np.array(post_pre['summary']),test_size=0.1,random_state=0,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQs4GkiuOxIR"
      },
      "outputs": [],
      "source": [
        "post_pre['summary'] = post_pre['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEyYBNJYO4H4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HB508LEPWa_",
        "outputId": "e5087ca1-575d-4951-a5aa-9937ec8ad715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 57.91270391131826\n",
            "Total Coverage of rare words: 1.3404923996005096\n"
          ]
        }
      ],
      "source": [
        "thresh=4\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "\n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmnqS20GPaza",
        "outputId": "e097786d-c42b-4c0d-b1e1-b7d620ffef92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary in X = 33412\n"
          ]
        }
      ],
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\n",
        "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr)\n",
        "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr = pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc = x_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in X = {}\".format(x_voc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXoyG9UgPiIJ"
      },
      "outputs": [],
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_tr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkbr-RMGPl8K",
        "outputId": "67352e94-cc20-43f9-fc52-29e827c2f8b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 66.34503603813067\n",
            "Total Coverage of rare words: 3.566630093901333\n"
          ]
        }
      ],
      "source": [
        "thresh=6\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "\n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E82Gs3DPq3k",
        "outputId": "12429514-ac79-4558-a884-d97a910fd9fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary in Y = 11581\n"
          ]
        }
      ],
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "#convert text sequences into integer sequences (i.e one hot encode the text in Y)\n",
        "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr)\n",
        "y_val_seq = y_tokenizer.texts_to_sequences(y_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "y_tr = pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "#size of vocabulary\n",
        "y_voc = y_tokenizer.num_words +1\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CepO6-VnPu2u"
      },
      "outputs": [],
      "source": [
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "    cnt=0\n",
        "    for j in y_tr[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr=np.delete(y_tr,ind, axis=0)\n",
        "x_tr=np.delete(x_tr,ind, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBrDbmcvP6no"
      },
      "outputs": [],
      "source": [
        "## Your code here ##\n",
        "## Design model train ##\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Attention, Concatenate, Input, Flatten, Dropout, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "def create_model(x_voc, y_voc, max_text_len, max_summary_len):\n",
        "    \"\"\"\n",
        "    Create a Seq2Seq model for text summarization.\n",
        "\n",
        "    Parameters:\n",
        "    x_voc (int): Size of the vocabulary for input text\n",
        "    y_voc (int): Size of the vocabulary for output summary\n",
        "    max_text_len (int): Maximum length of input text sequences\n",
        "    max_summary_len (int): Maximum length of output summary sequences\n",
        "\n",
        "    Returns:\n",
        "    model (tf.keras.Model): Compiled Seq2Seq model\n",
        "    \"\"\"\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_text_len,))\n",
        "    encoder_embedding = Embedding(x_voc, 128)(encoder_inputs)\n",
        "    encoder_lstm = Bidirectional(LSTM(128, return_sequences=True))(encoder_embedding)\n",
        "    \n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(max_summary_len,))\n",
        "    decoder_embedding = Embedding(y_voc, 128)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(128, return_sequences=True)(decoder_embedding)\n",
        "\n",
        "    # Attention layer\n",
        "    attention = Attention()([encoder_lstm, decoder_lstm])\n",
        "    \n",
        "    # Concatenate encoder and attention outputs\n",
        "    decoder_concat = Concatenate(axis=-1)([decoder_lstm, attention])\n",
        "    \n",
        "    # Output layer\n",
        "    outputs = TimeDistributed(Dense(y_voc, activation='softmax'))(decoder_concat)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiT6zZIKQ7LE"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x_tokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## set up optimizer and loss function ##\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Ensure x_voc and y_voc are defined\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m x_voc = \u001b[43mx_tokenizer\u001b[49m.num_words + \u001b[32m1\u001b[39m\n\u001b[32m      5\u001b[39m y_voc = y_tokenizer.num_words + \u001b[32m1\u001b[39m\n\u001b[32m      7\u001b[39m model = create_model(x_voc, y_voc, max_text_len, max_summary_len)\n",
            "\u001b[31mNameError\u001b[39m: name 'x_tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "## set up optimizer and loss function ##\n",
        "model = create_model(x_voc, y_voc, max_text_len, max_summary_len)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "loss_function = 'sparse_categorical_crossentropy'\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fKQVYU2RCQO",
        "outputId": "3e570853-310c-4170-a706-ece984afb9cb"
      },
      "outputs": [],
      "source": [
        "## Training ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrij3LGwROFK"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyplot\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pyplot.plot(\u001b[43mhistory\u001b[49m.history[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m pyplot.plot(history.history[\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m pyplot.legend()\n",
            "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
          ]
        }
      ],
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6Ao2SnuRWeY"
      },
      "outputs": [],
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4--ncOppRboU"
      },
      "outputs": [],
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A18GIsiERiIy"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "\n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP5d_I9fRl3E"
      },
      "outputs": [],
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGEZZlsORos1"
      },
      "outputs": [],
      "source": [
        "for i in range(0,100):\n",
        "    print(\"Review:\",seq2text(x_tr[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
        "    print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
