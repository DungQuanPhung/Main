{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "21e1e73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.24623879507347, -0.04492266828968239)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sympy import symbols, diff, sin\n",
    "\n",
    "def gradient_descent(f, lr, x_t, max_iter, tol):\n",
    "    #h = 1e-5  # Small step for numerical differentiation\n",
    "    #y_diff = (f(x0 + h) - f(x0)) / h\n",
    "    x = symbols('x')\n",
    "    f_sym = f(x)\n",
    "    y_diff = diff(f_sym, x)\n",
    "    i = 0\n",
    "    while i <= max_iter:\n",
    "        grad = float(y_diff.subs(x, x_t))\n",
    "        x_t1 = x_t - lr * grad\n",
    "        if np.abs(grad) < tol:\n",
    "            break\n",
    "        x_t = x_t1\n",
    "        i += 1\n",
    "\n",
    "    return f_sym.subs(x, x_t), grad\n",
    "\n",
    "gradient_descent(\n",
    "    f=lambda x: x**2 + 5*sin(x),\n",
    "    lr=0.1,\n",
    "    x_t=-10,\n",
    "    max_iter=100,\n",
    "    tol=1e-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b7acad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, diff, Matrix\n",
    "\n",
    "def gradient(f, vars):\n",
    "    grad = [diff(f, var) for var in vars]\n",
    "    return Matrix(grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ed7c7a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix([[4*x*(x**2 + y - 7) + 2*x - 2*y + 2], [2*x**2 - 2*x + 4*y - 16]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.83185466235675e-7, -2.999855012457979, -1.9992471632382347)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent_variables(f, lr, x_t, max_iter, tol):\n",
    "    # define vector gradient for multiple variables\n",
    "    x, y = symbols('x y')  # Define symbols for x and y\n",
    "    grad = gradient(f, [x, y])\n",
    "    print(grad)  # Use f_sym here\n",
    "    i = 0\n",
    "    while i <= max_iter:\n",
    "        grad_values = [float(grad[0].subs({x: x_t[0], y: x_t[1]})), float(grad[1].subs({x: x_t[0], y: x_t[1]}))]\n",
    "        x_t1 = [x_t[0] - lr * grad_values[0], x_t[1] - lr * grad_values[1]]\n",
    "        if np.linalg.norm(grad_values) < tol:\n",
    "            break\n",
    "        x_t = x_t1\n",
    "        i += 1\n",
    "\n",
    "    return f.subs({x: x_t[0], y: x_t[1]}), x_t[0], x_t[1]\n",
    "\n",
    "x, y = symbols('x y')  # Define symbols for x and y globally\n",
    "\n",
    "gradient_descent_variables(\n",
    "    f=(x**2 + y - 7)**2 + (x - y + 1)**2,\n",
    "    lr=0.015,\n",
    "    x_t=[0, 0],\n",
    "    max_iter=1000,\n",
    "    tol=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "69a2f365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nghiệm gần đúng: -2.9998812889419706\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient(f, x, eps=1e-8):\n",
    "    grad = np.zeros_like(x)\n",
    "    fx = f(x)\n",
    "    for i in range(len(x)):\n",
    "        dx = np.zeros_like(x)\n",
    "        dx[i] = eps\n",
    "        grad[i] = (f(x + dx) - fx) / eps\n",
    "    return grad\n",
    "\n",
    "def accelerated_gradient_descent(f, x0, lr, N, tol):\n",
    "    x_prev = np.copy(x0)\n",
    "    x = np.copy(x0)\n",
    "    i = 0\n",
    "\n",
    "    while i <= N - 1:\n",
    "        y = x + (i - 1) / (i + 2) * (x - x_prev)      \n",
    "        grad_y = gradient(f, y)\n",
    "        x_next = y - lr * grad_y\n",
    "\n",
    "        if np.linalg.norm(gradient(f, x_next)) < tol:\n",
    "            return x_next\n",
    "\n",
    "        x_prev = x\n",
    "        x = x_next\n",
    "        i += 1\n",
    "\n",
    "    return x\n",
    "\n",
    "f = lambda x: (x[0]**2 + x[1] - 7)**2 + (x[0] - x[1] + 1)**2\n",
    "\n",
    "x0 = np.array([0, 0])\n",
    "x_min, success = accelerated_gradient_descent(f, x0, lr=0.015, N=1000, tol=1e-3)\n",
    "\n",
    "print(\"Nghiệm gần đúng:\", x_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_448\\826700701.py:12: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  x_t2 = [f[0] - lr * grad[0], f[1] - lr * grad[1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(nan, np.float64(nan), np.float64(nan))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Accelerated_gradient_descent(f, lr, x_t, max_iter, tol):\n",
    "    # define vector gradient for multiple variables\n",
    "    x, y = symbols('x y')\n",
    "    grad = gradient(f, [x, y])\n",
    "    i = 0\n",
    "    x_t1 = x_t  # Initialize x_t1 with the initial value of x_t\n",
    "    while i <= max_iter - 1:\n",
    "        f = x_t + (i - 1)/(i + 2) * (np.array(x_t) - np.array(x_t1))\n",
    "        grad = [float(grad.subs({x: x_t[0], y: x_t[1]})), float(grad.subs({x: x_t[0], y: x_t[1]}))]\n",
    "        x_t2 = [f[0] - lr * grad[0], f[1] - lr * grad[1]]\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        x_t = x_t1\n",
    "        x_t1 = x_t2\n",
    "        i += 1\n",
    "\n",
    "    return f.subs({x: x_t[0], y: x_t[1]}), x_t[0], x_t[1]\n",
    "\n",
    "Accelerated_gradient_descent(\n",
    "    f=lambda x, y: (x**2 + y - 7)**2 + (x - y + 1)**2,\n",
    "    lr=0.015,\n",
    "    x_t=[0, 0],\n",
    "    max_iter=1000,\n",
    "    tol=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6db3eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   STT  Tuổi  HATT\n",
      "0    1     39   144\n",
      "1    2     36   136\n",
      "2    3     45   138\n",
      "3    4     47   145\n",
      "4    5     65   162\n",
      "Index(['STT', 'Tuổi', 'HATT'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-2.46944233e+241,  2.75781515e+240]), False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('tuoi_hatt.csv')\n",
    "print(df.head())\n",
    "# Chuyển dữ liệu từ DataFrame về numpy arrays\n",
    "# Inspect column names to ensure correct usage\n",
    "print(df.columns)\n",
    "\n",
    "# Use the correct column names\n",
    "a = df['Tuổi'].values.reshape(-1, 1)  # đặc trưng (feature)\n",
    "b = df['HATT'].values  # nhãn (label)\n",
    "\n",
    "# Thêm bias term vào a (thành [x, 1] để học cả hệ số chặn)\n",
    "a_bias = np.hstack([a, np.ones_like(a)])\n",
    "\n",
    "# Tạo danh sách loss function f_i và gradient tương ứng cho SGD\n",
    "f_list = []\n",
    "grad_list = []\n",
    "\n",
    "for i in range(len(a)):\n",
    "    ai = a_bias[i]\n",
    "    bi = b[i]\n",
    "    f_i = lambda x, ai=ai, bi=bi: 0.5 * (np.dot(ai, x) - bi)**2\n",
    "    grad_i = lambda x, ai=ai, bi=bi: ai * (np.dot(ai, x) - bi)\n",
    "    f_list.append(f_i)\n",
    "    grad_list.append(grad_i)\n",
    "\n",
    "# Hàm SGD đã định nghĩa trước đó\n",
    "def stochastic_gradient_descent(f_list, grad_list, x0, eta, N, epsilon=1e-6):\n",
    "    m = len(f_list)\n",
    "    x = np.copy(x0)\n",
    "    \n",
    "    for i in range(N):\n",
    "        idx = np.random.randint(0, m)\n",
    "        grad = grad_list[idx](x)\n",
    "        x_new = x - eta * grad\n",
    "\n",
    "        full_grad = np.sum([g(x_new) for g in grad_list], axis=0) / m\n",
    "        if np.linalg.norm(full_grad) < epsilon:\n",
    "            return x_new, True\n",
    "\n",
    "        x = x_new\n",
    "\n",
    "    return x, False\n",
    "\n",
    "# Gọi hàm SGD với vector khởi tạo x0 = [0, 0]\n",
    "x0 = np.zeros(a_bias.shape[1])\n",
    "x_result, success = stochastic_gradient_descent(f_list, grad_list, x0, eta=0.001, N=5000)\n",
    "\n",
    "x_result, success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b18217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.70210023313253\n"
     ]
    }
   ],
   "source": [
    "# Dự đoán tuổi khi biết huyết áp tâm thu\n",
    "X = df[\"HATT\"].values.reshape(-1, 1)\n",
    "y = df['Tuổi'].values\n",
    "\n",
    "X_bias = np.hstack([X, np.ones_like(X)])\n",
    "\n",
    "f_list_age = []\n",
    "grad_list_age = []\n",
    "\n",
    "for i in range(len(X_bias)):\n",
    "    xi = X_bias[i]\n",
    "    yi = y[i]\n",
    "    f_i = lambda w, xi=xi, yi=yi: 0.5 * (np.dot(xi, w) - yi)**2\n",
    "    grad_i = lambda w, xi=xi, yi=yi: xi * (np.dot(xi, w) - yi)\n",
    "    f_list_age.append(f_i)\n",
    "    grad_list_age.append(grad_i)\n",
    "\n",
    "w0 = np.zeros(X_bias.shape[1])\n",
    "w_result, success_age = stochastic_gradient_descent(f_list_age, grad_list_age, w0, eta=0.00001, N=5000)\n",
    "HATT = 150\n",
    "age = w_result[0]*HATT-w_result[1]\n",
    "print(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8bc5ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán HATT cho người 50 tuổi: 141.80691245488586\n"
     ]
    }
   ],
   "source": [
    "# Đảo lại: input là Tuổi, output là HATT\n",
    "X2 = df[\"Tuổi\"].values.reshape(-1, 1)  # input: Tuổi\n",
    "y2 = df[\"HATT\"].values  # output: HATT\n",
    "\n",
    "# Thêm bias vào input\n",
    "X2_bias = np.hstack([X2, np.ones_like(X2)])\n",
    "\n",
    "# Tạo danh sách hàm mất mát và gradient\n",
    "f_list_bp = []\n",
    "grad_list_bp = []\n",
    "\n",
    "for i in range(len(X2_bias)):\n",
    "    xi = X2_bias[i]\n",
    "    yi = y2[i]\n",
    "    f_i = lambda w, xi=xi, yi=yi: 0.5 * (np.dot(xi, w) - yi)**2\n",
    "    grad_i = lambda w, xi=xi, yi=yi: xi * (np.dot(xi, w) - yi)\n",
    "    f_list_bp.append(f_i)\n",
    "    grad_list_bp.append(grad_i)\n",
    "\n",
    "# Vector trọng số khởi tạo\n",
    "w0_bp = np.zeros(X2_bias.shape[1])\n",
    "\n",
    "# Chạy SGD để huấn luyện mô hình dự đoán HATT từ tuổi\n",
    "w_result_bp, success_bp = stochastic_gradient_descent(f_list_bp, grad_list_bp, w0_bp, eta=0.00001, N=5000)\n",
    "\n",
    "w_result_bp, success_bp\n",
    "tuoi_test = 50\n",
    "hatt_du_doan = w_result_bp[0] * tuoi_test + w_result_bp[1]\n",
    "print(f\"Dự đoán HATT cho người {tuoi_test} tuổi:\", hatt_du_doan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f092dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.83185466235675e-7, -2.999855012457979, -1.9992471632382347)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Stochastic_gradient_descent(a, b, x_t, max_iter, tol):\n",
    "    # define vector gradient for multiple variables\n",
    "    x, y = symbols('x y')\n",
    "    f_sym = f(x, y)\n",
    "    grad_x = diff(f_sym, x)\n",
    "    grad_y = diff(f_sym, y)\n",
    "    i = 0\n",
    "    while i <= max_iter - 1:\n",
    "        grad = [float(grad_x.subs({x: x_t[0], y: x_t[1]})), float(grad_y.subs({x: x_t[0], y: x_t[1]}))]\n",
    "        x_t1 = [x_t[0] - lr * grad[0], x_t[1] - lr * grad[1]]\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        x_t = x_t1\n",
    "        i += 1\n",
    "\n",
    "    return f_sym.subs({x: x_t[0], y: x_t[1]}), x_t[0], x_t[1]\n",
    "\n",
    "Stochastic_gradient_descent(\n",
    "    f=lambda x, y: (x**2 + y - 7)**2 + (x - y + 1)**2,\n",
    "    lr=0.015,\n",
    "    x_t=[0, 0],\n",
    "    max_iter=1000,\n",
    "    tol=1e-3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
