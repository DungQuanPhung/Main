{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21e1e73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.24623879507347, -0.04492266828968239)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sympy import symbols, diff, sin\n",
    "\n",
    "def gradient_descent(f, lr, x_t, max_iter, tol):\n",
    "    #h = 1e-5  # Small step for numerical differentiation\n",
    "    #y_diff = (f(x0 + h) - f(x0)) / h\n",
    "    x = symbols('x')\n",
    "    f_sym = f(x)\n",
    "    y_diff = diff(f_sym, x)\n",
    "    i = 0\n",
    "    while i <= max_iter:\n",
    "        grad = float(y_diff.subs(x, x_t))\n",
    "        x_t1 = x_t - lr * grad\n",
    "        if np.abs(grad) < tol:\n",
    "            break\n",
    "        x_t = x_t1\n",
    "        i += 1\n",
    "\n",
    "    return f_sym.subs(x, x_t), grad\n",
    "\n",
    "gradient_descent(\n",
    "    f=lambda x: x**2 + 5*sin(x),\n",
    "    lr=0.1,\n",
    "    x_t=-10,\n",
    "    max_iter=100,\n",
    "    tol=1e-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed7c7a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.83185466235675e-7, -2.999855012457979, -1.9992471632382347)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent_variables(f, lr, x_t, max_iter, tol):\n",
    "    # define vector gradient for multiple variables\n",
    "    x, y = symbols('x y')\n",
    "    f_sym = f(x, y)\n",
    "    grad_x = diff(f_sym, x)\n",
    "    grad_y = diff(f_sym, y)\n",
    "    i = 0\n",
    "    while i <= max_iter:\n",
    "        grad = [float(grad_x.subs({x: x_t[0], y: x_t[1]})), float(grad_y.subs({x: x_t[0], y: x_t[1]}))]\n",
    "        x_t1 = [x_t[0] - lr * grad[0], x_t[1] - lr * grad[1]]\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        x_t = x_t1\n",
    "        i += 1\n",
    "\n",
    "    return f_sym.subs({x: x_t[0], y: x_t[1]}), x_t[0], x_t[1]\n",
    "\n",
    "gradient_descent_variables(\n",
    "    f=lambda x, y: (x**2 + y - 7)**2 + (x - y + 1)**2,\n",
    "    lr=0.015,\n",
    "    x_t=[0, 0],\n",
    "    max_iter=1000,\n",
    "    tol=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a668660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_448\\826700701.py:12: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  x_t2 = [f[0] - lr * grad[0], f[1] - lr * grad[1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(nan, np.float64(nan), np.float64(nan))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Accelerated_gradient_descent(f, lr, x_t, max_iter, tol):\n",
    "    # define vector gradient for multiple variables\n",
    "    x, y = symbols('x y')\n",
    "    f_sym = f(x, y)\n",
    "    grad_x = diff(f_sym, x)\n",
    "    grad_y = diff(f_sym, y)\n",
    "    i = 0\n",
    "    x_t1 = x_t  # Initialize x_t1 with the initial value of x_t\n",
    "    while i <= max_iter - 1:\n",
    "        f = x_t + (i - 1)/(i + 2) * (np.array(x_t) - np.array(x_t1))\n",
    "        grad = [float(grad_x.subs({x: x_t[0], y: x_t[1]})), float(grad_y.subs({x: x_t[0], y: x_t[1]}))]\n",
    "        x_t2 = [f[0] - lr * grad[0], f[1] - lr * grad[1]]\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        x_t = x_t1\n",
    "        x_t1 = x_t2\n",
    "        i += 1\n",
    "\n",
    "    return f_sym.subs({x: x_t[0], y: x_t[1]}), x_t[0], x_t[1]\n",
    "\n",
    "Accelerated_gradient_descent(\n",
    "    f=lambda x, y: (x**2 + y - 7)**2 + (x - y + 1)**2,\n",
    "    lr=0.015,\n",
    "    x_t=[0, 0],\n",
    "    max_iter=1000,\n",
    "    tol=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f092dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.83185466235675e-7, -2.999855012457979, -1.9992471632382347)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Stochastic_gradient_descent(a, b, x_t, max_iter, tol):\n",
    "    # define vector gradient for multiple variables\n",
    "    x, y = symbols('x y')\n",
    "    f_sym = f(x, y)\n",
    "    grad_x = diff(f_sym, x)\n",
    "    grad_y = diff(f_sym, y)\n",
    "    i = 0\n",
    "    while i <= max_iter - 1:\n",
    "        grad = [float(grad_x.subs({x: x_t[0], y: x_t[1]})), float(grad_y.subs({x: x_t[0], y: x_t[1]}))]\n",
    "        x_t1 = [x_t[0] - lr * grad[0], x_t[1] - lr * grad[1]]\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        x_t = x_t1\n",
    "        i += 1\n",
    "\n",
    "    return f_sym.subs({x: x_t[0], y: x_t[1]}), x_t[0], x_t[1]\n",
    "\n",
    "Stochastic_gradient_descent(\n",
    "    f=lambda x, y: (x**2 + y - 7)**2 + (x - y + 1)**2,\n",
    "    lr=0.015,\n",
    "    x_t=[0, 0],\n",
    "    max_iter=1000,\n",
    "    tol=1e-3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
