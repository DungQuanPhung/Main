{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "Vietnamese Students' Feedback Corpus (UIT-VSFC) is the resource consists of over 16,000 sentences which are human-annotated with two different tasks: sentiment-based and topic-based classifications.\n",
    "\n",
    "[1] Kiet Van Nguyen, Vu Duc Nguyen, Phu Xuan-Vinh Nguyen, Tham Thi-Hong Truong, Ngan Luu-Thuy Nguyen, UIT-VSFC: Vietnamese Students' Feedback Corpus for Sentiment Analysis,  2018 10th International Conference on Knowledge and Systems Engineering (KSE 2018), November 1-3, 2018, Ho Chi Minh City, Vietnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"uitnlp/vietnamese_students_feedback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentiment', 'topic'],\n",
       "    num_rows: 11426\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = dataset['train']\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'slide giáo trình đầy đủ .', 'sentiment': 2, 'topic': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11426"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slide giáo trình đầy đủ .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a sentence\n",
    "example_word_list = train_set[0]['sentence']\n",
    "example_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slide', 'giáo', 'trình', 'đầy', 'đủ', '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split sentence word-by-word\n",
    "example_word_list.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slide giáo trình đầy đủ .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join words into 1 full sentence\n",
    "sentence = \"\"\n",
    "for word in example_word_list:\n",
    "    sentence += word\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slide giáo trình đầy đủ .',\n",
       " 'nhiệt tình giảng dạy , gần gũi với sinh viên .',\n",
       " 'đi học đầy đủ full điểm chuyên cần .',\n",
       " 'chưa áp dụng công nghệ thông tin và các thiết bị hỗ trợ cho việc giảng dạy .',\n",
       " 'thầy giảng bài hay , có nhiều bài tập ví dụ ngay trên lớp .',\n",
       " 'giảng viên đảm bảo thời gian lên lớp , tích cực trả lời câu hỏi của sinh viên , thường xuyên đặt câu hỏi cho sinh viên .',\n",
       " 'em sẽ nợ môn này , nhưng em sẽ học lại ở các học kỳ kế tiếp .',\n",
       " 'thời lượng học quá dài , không đảm bảo tiếp thu hiệu quả .',\n",
       " 'nội dung môn học có phần thiếu trọng tâm , hầu như là chung chung , khái quát khiến sinh viên rất khó nắm được nội dung môn học .',\n",
       " 'cần nói rõ hơn bằng cách trình bày lên bảng thay vì nhìn vào slide .']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 10 sentences to process\n",
    "sentence_list = []\n",
    "for idx in range(10):\n",
    "    sentence = \"\"\n",
    "    for word in train_set[idx]['sentence']:\n",
    "        sentence += word\n",
    "    sentence_list.append(sentence)\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "- N-grams are continuous sequences of words or symbols, or tokens in a document. In technical terms, they can be defined as the neighboring sequences of items in a document.\n",
    "- We can use n-grams or multiple other text preprocessing algorithms by incorporating [`nltk`](https://www.nltk.org/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: This document is the second document.\n",
      "===============\n",
      "1-gram: ['This', 'document', 'is', 'the', 'second', 'document.']\n",
      "\n",
      "2-gram: ['This document', 'document is', 'is the', 'the second', 'second document.']\n",
      "\n",
      "3-gram: ['This document is', 'document is the', 'is the second', 'the second document.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "\n",
    "num_of_grams = np.arange(1, 4, 1) # Test 3 n-grams\n",
    "\n",
    "print(\"Original sentence:\", example_sentence[1])\n",
    "print(\"===\"*5)\n",
    "\n",
    "for gram in num_of_grams:\n",
    "    splitted_sentence = ngrams(example_sentence[1].split(), int(gram))\n",
    "    print(f\"{gram}-gram: \",end ='')\n",
    "    n_grams_list = [' '.join(grams) for grams in splitted_sentence]\n",
    "    print(n_grams_list)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features with n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1  2  3\n",
       "and       0  0  1  0\n",
       "document  1  2  0  1\n",
       "first     1  0  0  1\n",
       "is        1  1  1  1\n",
       "one       0  0  1  0\n",
       "second    0  1  0  0\n",
       "the       1  1  1  1\n",
       "third     0  0  1  0\n",
       "this      1  1  1  1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 1))\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform(example_sentence).toarray()\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "word_frequency.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: giảng viên đảm bảo thời gian lên lớp , tích cực trả lời câu hỏi của sinh viên , thường xuyên đặt câu hỏi cho sinh viên .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bảo</th>\n",
       "      <th>cho</th>\n",
       "      <th>câu</th>\n",
       "      <th>của</th>\n",
       "      <th>cực</th>\n",
       "      <th>gian</th>\n",
       "      <th>giảng</th>\n",
       "      <th>hỏi</th>\n",
       "      <th>lên</th>\n",
       "      <th>lớp</th>\n",
       "      <th>lời</th>\n",
       "      <th>sinh</th>\n",
       "      <th>thường</th>\n",
       "      <th>thời</th>\n",
       "      <th>trả</th>\n",
       "      <th>tích</th>\n",
       "      <th>viên</th>\n",
       "      <th>xuyên</th>\n",
       "      <th>đảm</th>\n",
       "      <th>đặt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bảo  cho  câu  của  cực  gian  giảng  hỏi  lên  lớp  lời  sinh  thường  \\\n",
       "0    1    1    2    1    1     1      1    2    1    1    1     2       1   \n",
       "\n",
       "   thời  trả  tích  viên  xuyên  đảm  đặt  \n",
       "0     1    1     1     3      1    1    1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 1))\n",
    "\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform([sentence_list[5]]).toarray()\n",
    "\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "\n",
    "print('Example sentence:', sentence_list[5])\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and this</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document is</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first document</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is this</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this is</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this the</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0  1  2  3\n",
       "and              0  0  1  0\n",
       "and this         0  0  1  0\n",
       "document         1  2  0  1\n",
       "document is      0  1  0  0\n",
       "first            1  0  0  1\n",
       "first document   1  0  0  1\n",
       "is               1  1  1  1\n",
       "is the           1  1  1  0\n",
       "is this          0  0  0  1\n",
       "one              0  0  1  0\n",
       "second           0  1  0  0\n",
       "second document  0  1  0  0\n",
       "the              1  1  1  1\n",
       "the first        1  0  0  1\n",
       "the second       0  1  0  0\n",
       "the third        0  0  1  0\n",
       "third            0  0  1  0\n",
       "third one        0  0  1  0\n",
       "this             1  1  1  1\n",
       "this document    0  1  0  0\n",
       "this is          1  0  1  0\n",
       "this the         0  0  0  1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 2))\n",
    "\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform(example_sentence).toarray()\n",
    "\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "word_frequency.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 1\n",
    "Based on the UIT-VSFC dataset and the aforementioned information.\n",
    "- Create an $n$-gram word frequency table, such that $n$ could be any number of your desire.\n",
    "- With $n=1$ and $n=2$, what is the most popular word in the dataset ?\n",
    "- With $n=1$ and $n=2$, what is the rarest word in the dataset ?\n",
    "- What are the limitations of this data processing flow ? How can we overcome those ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve all sentences within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_all_sentences(dataset) -> List[str]:\n",
    "    \"\"\"\n",
    "    Function to get all sentences and store them into a list of strings\n",
    "\n",
    "    Args:\n",
    "    dataset -- The subset (i.e., train/valid/test) in UIT-VSFC dataset\n",
    "\n",
    "    Returns:\n",
    "    A list of all sentences in a subset data of the UIT-VSFC.\n",
    "    \"\"\"\n",
    "\n",
    "    list_all_sentence: list = []\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    for idx in range(len(dataset)):\n",
    "        sentence = \"\"\n",
    "        for word in dataset[idx]['sentence']:\n",
    "            sentence += word\n",
    "        list_all_sentence.append(sentence)\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return list_all_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences within the dataset: 11426\n",
      "Example sentence: slide giáo trình đầy đủ .\n"
     ]
    }
   ],
   "source": [
    "list_all_sentence: list = get_all_sentences(train_set)\n",
    "print(f\"#sentences within the dataset: {len(list_all_sentence)}\")\n",
    "print(f\"Example sentence: {list_all_sentence[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the word frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_word_frequency(sentence_list: list,\n",
    "                          n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to build a word frequency table based on n-grams\n",
    "\n",
    "    Args:\n",
    "    sentence_list (list) -- A list of all sentences needed for table constructing process\n",
    "    n (int) -- Number of grams that we parse into this function\n",
    "\n",
    "    Returns:\n",
    "    A dataframe contains all words after conducting n-grams and their respective frequencies\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    count_vectorize_model = CountVectorizer(ngram_range = (n, n))\n",
    "    n_grams_feature_vector = count_vectorize_model.fit_transform(sentence_list).toarray()\n",
    "    word_frequency_table = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return word_frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Frequency\n",
      "viên        4803\n",
      "giảng       3711\n",
      "dạy         3156\n",
      "thầy        3095\n",
      "sinh        3082\n",
      "học         2940\n",
      "bài         2336\n",
      "tình        2266\n",
      "không       2177\n",
      "và          2068\n",
      "Từ phổ biến nhất:\n",
      " viên\n",
      "               Frequency\n",
      "dọa                    1\n",
      "đếm                    1\n",
      "đế                     1\n",
      "gán                    1\n",
      "ấm                     1\n",
      "ướt                    1\n",
      "ức                     1\n",
      "đống                   1\n",
      "đốn                    1\n",
      "11doubledot55          1\n",
      "Từ hiếm gặp nhất:\n",
      " 11doubledot55\n"
     ]
    }
   ],
   "source": [
    "# Construct the table of word frequency\n",
    "# 1-ngram\n",
    "word_frequency_table_1ngram = n_gram_word_frequency(sentence_list=list_all_sentence,\n",
    "                                             n=1)\n",
    "word_frequency_1ngram = word_frequency_table_1ngram.sum(axis=0).sort_values(ascending=False).to_frame('Frequency')\n",
    "\n",
    "print(word_frequency_1ngram[:10])\n",
    "print(\"Từ phổ biến nhất:\\n\", word_frequency_1ngram.index[0])\n",
    "print(word_frequency_1ngram[-10:])\n",
    "print(\"Từ hiếm gặp nhất:\\n\", word_frequency_1ngram.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Frequency\n",
      "sinh viên        2698\n",
      "nhiệt tình       1848\n",
      "giảng viên       1610\n",
      "bài tập          1057\n",
      "dễ hiểu          1004\n",
      "giảng dạy         956\n",
      "kiến thức         904\n",
      "thực hành         877\n",
      "môn học           688\n",
      "cho sinh          656\n",
      "Từ phổ biến nhất:\n",
      " sinh viên\n",
      "                  Frequency\n",
      "100 cách                  1\n",
      "100 là                    1\n",
      "100 người                 1\n",
      "100 tự                    1\n",
      "10h mới                   1\n",
      "10h30 nhưng               1\n",
      "11 thì                    1\n",
      "11doubledot55 pm          1\n",
      "11h30 nghỉ                1\n",
      "trên google               1\n",
      "Từ hiếm gặp nhất:\n",
      " trên google\n"
     ]
    }
   ],
   "source": [
    "# 2-ngram\n",
    "word_frequency_table_2ngram = n_gram_word_frequency(sentence_list=list_all_sentence,\n",
    "                                             n=2)\n",
    "word_frequency_2ngram = word_frequency_table_2ngram.sum(axis=0).sort_values(ascending=False).to_frame('Frequency')\n",
    "\n",
    "print(word_frequency_2ngram[:10])\n",
    "print(\"Từ phổ biến nhất:\\n\", word_frequency_2ngram.index[0])\n",
    "print(word_frequency_2ngram[-10:])\n",
    "print(\"Từ hiếm gặp nhất:\\n\", word_frequency_2ngram.index[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You should comment your answer to problem 1 here with sufficient explanations, including your implementation and reasoning.\n",
    "\n",
    "- With n = 1, the most popular word is \"viên\" appearing 4803 times, the rarest word is \"11doubledot55\" appearing 1 time. However, there are still many words appearing 1 time. Eg: \"đốn\", \"đống\", \"ức\".\n",
    "- With n = 2, the most popular word is \"sinh viên\" appearing 2698 times, the rarest word is \"trên google\" appearing 1 time. However, there are still many words appearing 1 time. Eg: \"11h30 nghỉ\", \"11doubledot55 pm\", \"11 thì\".\n",
    "- The limitations of this data processing flow is stopwords may take many quantity, it will reduce the important of remaining words though they are really needed.\n",
    "- Mispelling is also a problem because it will disperse frequency of words. Eg: \"sinhviên\" and \"sinh viên\" may be separated count with different meaning.\n",
    "- The computation cost will be large if the 'n' large. It also take much time to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the stopword dictionary\n",
    "import wget\n",
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Number of stop words: 1942\n"
     ]
    }
   ],
   "source": [
    "# Observe stopwords list\n",
    "vietnamese_stopword = open('vietnamese-stopwords.txt', 'r', encoding='utf-8').read()\n",
    "vietnamese_stopword = vietnamese_stopword.split('\\n') # Separate lines by lines\n",
    "print(f\"#Number of stop words: {len(vietnamese_stopword)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a lô\n",
      "a ha\n",
      "ai\n",
      "ai ai\n",
      "ai nấy\n",
      "ai đó\n",
      "alô\n",
      "amen\n",
      "anh\n",
      "anh ấy\n"
     ]
    }
   ],
   "source": [
    "# Stop words example\n",
    "for sentence in vietnamese_stopword[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency - Invert document frequency (TF-IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF\n",
    "Term frequency (TF) is the number of times a given term appears in document\n",
    "\n",
    "$$\n",
    "tf(t) = f(t,d)\\times\\frac{1}{T}\n",
    "$$\n",
    "whereas, $f(t,d)$ is the frequency of the word $t$ in the document $d$, $T$ is the number of all words in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf\n",
       "and       0.0\n",
       "document  0.2\n",
       "first     0.2\n",
       "is        0.2\n",
       "one       0.0\n",
       "second    0.0\n",
       "the       0.2\n",
       "third     0.0\n",
       "this      0.2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Declare TF vectorize\n",
    "tf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                use_idf=False, # only using TF\n",
    "                                norm='l1')\n",
    "\n",
    "tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf_vectorized = tf_vectorizer.transform(corpus)\n",
    "\n",
    "tf_output = tf_vectorized[0]\n",
    "\n",
    "# Build TF table\n",
    "words_tf_idf = pd.DataFrame(tf_output.T.todense(), index=tf_vectorizer.get_feature_names_out(), columns=['tf'])\n",
    "words_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF\n",
    "\n",
    "Inverse Document Frequency, or abbreviated as IDF, measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones.\n",
    "\n",
    "$$\n",
    "idf(t) = \\log\\left(\\frac{\\text{#documents in the document set}}{\\text{#documents with term}}\\right) + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf       idf\n",
       "is        0.2  1.000000\n",
       "the       0.2  1.000000\n",
       "this      0.2  1.000000\n",
       "document  0.2  1.287682\n",
       "first     0.2  1.693147\n",
       "and       0.0  2.386294\n",
       "second    0.0  2.386294\n",
       "one       0.0  2.386294\n",
       "third     0.0  2.386294"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Configure settings for IDF vectorize\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                    smooth_idf=False,\n",
    "                                    use_idf=True,\n",
    "                                    norm=None)\n",
    "\n",
    "tf_idf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Retrieve only idf information\n",
    "idf_vectorizer = tf_idf_vectorizer.idf_\n",
    "\n",
    "# Join idf values into the previous dataframe\n",
    "words_tf_idf['idf'] = idf_vectorizer\n",
    "\n",
    "# Show dataframe with ascending values of idf\n",
    "words_tf_idf.sort_values(by=['idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Technically saying, TF-IDF is a score which is applied to every word in every document in our dataset. And for every word, the TF-IDF value increases with every appearance of the word in a document, but is gradually decreased with every appearance in other documents\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}= tf(t, d) \\times idf(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.215302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.283096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf       idf    tf-idf\n",
       "and       0.0  2.386294  0.000000\n",
       "third     0.0  2.386294  0.000000\n",
       "second    0.0  2.386294  0.000000\n",
       "one       0.0  2.386294  0.000000\n",
       "is        0.2  1.000000  0.167201\n",
       "the       0.2  1.000000  0.167201\n",
       "this      0.2  1.000000  0.167201\n",
       "document  0.2  1.287682  0.215302\n",
       "first     0.2  1.693147  0.283096"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                    smooth_idf=False,\n",
    "                                    use_idf=True,\n",
    "                                    norm='l1')\n",
    "\n",
    "tf_idf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf_idf_vectorized = tf_idf_vectorizer.transform(corpus)\n",
    "\n",
    "tf_idf_output = tf_idf_vectorized[0]\n",
    "words_tf_idf['tf-idf'] = tf_idf_output.T.todense()\n",
    "\n",
    "words_tf_idf.sort_values(by=['tf-idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem set 2\n",
    "Based on the problem 1 and the instruction on TF, IDF, TF-IDF:\n",
    "- (2a) Build the tf-idf table for the UIT-VSFC dataset with $n$-gram = 1 and $n$-gram = 2.\n",
    "- (2b) Change a few hyperparameters in the `TfidfVectorizer` function (`smooth_idf`, `sublinear_tf` and `norm`) from problem 2a (*you could browse from this [link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to discover which are the correct paramters to parse*). Explain the results differences collected after modifying hyperparameters.\n",
    "- (2c) Which words has the lowest and the highest tf-idf values ? Do they differ from $n$-grams results ?\n",
    "- (2d) Which limitations from $n$-grams that TF-IDF overcame ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Build the tf-idf table for the UIT-VSFC dataset with n-gram = 1 and n-gram = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 1-gram:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>10h</th>\n",
       "      <th>10h30</th>\n",
       "      <th>11</th>\n",
       "      <th>11doubledot55</th>\n",
       "      <th>11h30</th>\n",
       "      <th>11h55</th>\n",
       "      <th>12</th>\n",
       "      <th>12doubledot00</th>\n",
       "      <th>...</th>\n",
       "      <th>ấy</th>\n",
       "      <th>ẩn</th>\n",
       "      <th>ắt</th>\n",
       "      <th>ốc</th>\n",
       "      <th>ồn</th>\n",
       "      <th>ổn</th>\n",
       "      <th>ủa</th>\n",
       "      <th>ủng</th>\n",
       "      <th>ức</th>\n",
       "      <th>ứng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10  100  10h  10h30   11  11doubledot55  11h30  11h55   12  12doubledot00  \\\n",
       "0  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "1  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "2  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "3  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "4  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "5  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "6  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "7  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "8  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "9  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "\n",
       "   ...   ấy   ẩn   ắt   ốc   ồn   ổn   ủa  ủng   ức  ứng  \n",
       "0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "6  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "7  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "8  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[10 rows x 2459 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1-gram\n",
    "tfidf_vectorizer_1gram = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf_1gram = tfidf_vectorizer_1gram.fit_transform(list_all_sentence)\n",
    "tfidf_1gram_df = pd.DataFrame(tfidf_1gram.toarray(), columns=tfidf_vectorizer_1gram.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF with 1-gram:\")\n",
    "tfidf_1gram_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 2-gram:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 50</th>\n",
       "      <th>10 bài</th>\n",
       "      <th>10 fraction</th>\n",
       "      <th>10 kiến</th>\n",
       "      <th>10 luôn</th>\n",
       "      <th>10 mấy</th>\n",
       "      <th>10 mới</th>\n",
       "      <th>10 người</th>\n",
       "      <th>10 năm</th>\n",
       "      <th>10 phút</th>\n",
       "      <th>...</th>\n",
       "      <th>ứng kịp</th>\n",
       "      <th>ứng nhu</th>\n",
       "      <th>ứng nhưng</th>\n",
       "      <th>ứng tốt</th>\n",
       "      <th>ứng yêu</th>\n",
       "      <th>ứng đáp</th>\n",
       "      <th>ứng đúng</th>\n",
       "      <th>ứng được</th>\n",
       "      <th>ứng đầy</th>\n",
       "      <th>ứng đủ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10 50  10 bài  10 fraction  10 kiến  10 luôn  10 mấy  10 mới  10 người  \\\n",
       "0    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "1    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "2    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "3    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "4    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "5    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "6    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "7    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "8    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "9    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "\n",
       "   10 năm  10 phút  ...  ứng kịp  ứng nhu  ứng nhưng  ứng tốt  ứng yêu  \\\n",
       "0     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "1     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "2     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "3     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "4     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "5     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "6     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "7     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "8     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "9     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "\n",
       "   ứng đáp  ứng đúng  ứng được  ứng đầy  ứng đủ  \n",
       "0      0.0       0.0       0.0      0.0     0.0  \n",
       "1      0.0       0.0       0.0      0.0     0.0  \n",
       "2      0.0       0.0       0.0      0.0     0.0  \n",
       "3      0.0       0.0       0.0      0.0     0.0  \n",
       "4      0.0       0.0       0.0      0.0     0.0  \n",
       "5      0.0       0.0       0.0      0.0     0.0  \n",
       "6      0.0       0.0       0.0      0.0     0.0  \n",
       "7      0.0       0.0       0.0      0.0     0.0  \n",
       "8      0.0       0.0       0.0      0.0     0.0  \n",
       "9      0.0       0.0       0.0      0.0     0.0  \n",
       "\n",
       "[10 rows x 31384 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-ngram\n",
    "tfidf_vectorizer_2gram = TfidfVectorizer(ngram_range=(2, 2))\n",
    "tfidf_2gram = tfidf_vectorizer_2gram.fit_transform(list_all_sentence)\n",
    "tfidf_2gram_df = pd.DataFrame(tfidf_2gram.toarray(), columns=tfidf_vectorizer_2gram.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF with 2-gram:\")\n",
    "tfidf_2gram_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Change a few hyperparameters in the TfidfVectorizer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 1-gram:\n",
      "           TF-IDF\n",
      "viên   659.324137\n",
      "giảng  658.334312\n",
      "dạy    616.080713\n",
      "tình   558.548969\n",
      "thầy   552.767907\n",
      "        TF-IDF\n",
      "tá    0.125334\n",
      "vạ    0.120081\n",
      "gạo   0.120081\n",
      "ùn    0.120081\n",
      "lõng  0.120081\n",
      "TF-IDF with 1-gram (new hyperparameters):\n",
      "           TF-IDF\n",
      "dạy    230.719080\n",
      "giảng  230.481666\n",
      "tình   211.496246\n",
      "viên   205.576416\n",
      "thầy   198.167823\n",
      "        TF-IDF\n",
      "hỏa   0.023056\n",
      "lõng  0.016674\n",
      "gạo   0.016674\n",
      "ùn    0.016674\n",
      "vạ    0.016674\n"
     ]
    }
   ],
   "source": [
    "# 1-ngram\n",
    "new_tfidf_vectorizer_1gram = TfidfVectorizer(ngram_range=(1, 1), smooth_idf=False, sublinear_tf=True, norm='l1')\n",
    "new_tfidf_1gram = new_tfidf_vectorizer_1gram.fit_transform(list_all_sentence)\n",
    "new_tfidf_1gram_df = pd.DataFrame(new_tfidf_1gram.toarray(), columns=new_tfidf_vectorizer_1gram.get_feature_names_out())\n",
    "new_tfidf_sort_1gram = new_tfidf_1gram_df.sum(axis=0).sort_values(ascending=False).to_frame('TF-IDF')\n",
    "tfidf_sort_1gram = tfidf_1gram_df.sum(axis=0).sort_values(ascending=False).to_frame('TF-IDF')\n",
    "\n",
    "print(\"TF-IDF with 1-gram:\")\n",
    "print(tfidf_sort_1gram.head(5))\n",
    "print(tfidf_sort_1gram[-5:])\n",
    "print(\"TF-IDF with 1-gram (new hyperparameters):\")\n",
    "print(new_tfidf_sort_1gram.head(5))\n",
    "print(new_tfidf_sort_1gram[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 2-gram:\n",
      "                TF-IDF\n",
      "nhiệt tình  327.391256\n",
      "sinh viên   266.909184\n",
      "giảng viên  234.135877\n",
      "dễ hiểu     211.014566\n",
      "giảng dạy   179.107510\n",
      "            TF-IDF\n",
      "sức và    0.095209\n",
      "cả tên    0.095209\n",
      "lõng mơ   0.095209\n",
      "buộc thì  0.095209\n",
      "vạ gạo    0.095209\n",
      "TF-IDF with 2-gram (new hyperparameters):\n",
      "                TF-IDF\n",
      "nhiệt tình  134.238251\n",
      "dễ hiểu      84.110376\n",
      "giảng viên   82.846095\n",
      "sinh viên    78.990046\n",
      "giảng dạy    67.601236\n",
      "              TF-IDF\n",
      "buổi thông  0.008826\n",
      "mềm hệ      0.008826\n",
      "như phần    0.008826\n",
      "một vạ      0.008826\n",
      "gạo một     0.008826\n"
     ]
    }
   ],
   "source": [
    "# 2-ngram\n",
    "new_tfidf_vectorizer_2gram = TfidfVectorizer(ngram_range=(2, 2), smooth_idf=False, sublinear_tf=True, norm='l1')\n",
    "new_tfidf_2gram = new_tfidf_vectorizer_2gram.fit_transform(list_all_sentence)\n",
    "new_tfidf_2gram_df = pd.DataFrame(new_tfidf_2gram.toarray(), columns=new_tfidf_vectorizer_2gram.get_feature_names_out())\n",
    "new_tfidf_sort_2gram = new_tfidf_2gram_df.sum(axis=0).sort_values(ascending=False).to_frame('TF-IDF')\n",
    "tfidf_sort_2gram = tfidf_2gram_df.sum(axis=0).sort_values(ascending=False).to_frame('TF-IDF')\n",
    "\n",
    "print(\"TF-IDF with 2-gram:\")\n",
    "print(tfidf_sort_2gram.head(5))\n",
    "print(tfidf_sort_2gram[-5:])\n",
    "print(\"TF-IDF with 2-gram (new hyperparameters):\")\n",
    "print(new_tfidf_sort_2gram.head(5))\n",
    "print(new_tfidf_sort_2gram[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the results differences collected after modifying hyperparameters.\n",
    "- Change 'smooth_idf' from True(default) into False to reduce weight of words.\n",
    "- Change 'sublinear_tf' from False(default) into True to reduce influence of word which appear many times in one document. Help to balance out their influence across different documents.\n",
    "- Change 'norm' from l2(default) into l1, TF-IDF values sum to 1 per document instead of being scaled by Euclidean norm, which may affect how documents are compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Find words with the lowest and highest tf-idf values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams:\n",
      "Lowest 1-gram: 11doubledot55\n",
      "Highest 1-gram: viên\n",
      "TF-IDF with 1-gram:\n",
      "Lowest 1-gram TF-IDF: lõng\n",
      "Highest 1-gram TF-IDF: viên\n",
      "TF-IDF with 1-gram (new hyperparameters):\n",
      "Lowest 1-gram TF-IDF: vạ\n",
      "Highest 1-gram TF-IDF: dạy\n"
     ]
    }
   ],
   "source": [
    "# 1-ngram\n",
    "print(\"N-grams:\")\n",
    "print(\"Lowest 1-gram:\", word_frequency_1ngram.index[-1])\n",
    "print(\"Highest 1-gram:\", word_frequency_1ngram.index[0])\n",
    "print(\"TF-IDF with 1-gram:\")\n",
    "print(\"Lowest 1-gram TF-IDF:\", tfidf_sort_1gram.index[-1])\n",
    "print(\"Highest 1-gram TF-IDF:\", tfidf_sort_1gram.index[0])\n",
    "print(\"TF-IDF with 1-gram (new hyperparameters):\")\n",
    "print(\"Lowest 1-gram TF-IDF:\", new_tfidf_sort_1gram.index[-1])\n",
    "print(\"Highest 1-gram TF-IDF:\", new_tfidf_sort_1gram.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams:\n",
      "Lowest 2-gram: trên google\n",
      "Highest 2-gram: sinh viên\n",
      "TF-IDF with 2-gram:\n",
      "Lowest 2-gram TF-IDF: vạ gạo\n",
      "Highest 2-gram TF-IDF: nhiệt tình\n",
      "TF-IDF with 2-gram (new hyperparameters):\n",
      "Lowest 2-gram TF-IDF: gạo một\n",
      "Highest 2-gram TF-IDF: nhiệt tình\n"
     ]
    }
   ],
   "source": [
    "# 2-ngram\n",
    "print(\"N-grams:\")\n",
    "print(\"Lowest 2-gram:\", word_frequency_2ngram.index[-1])\n",
    "print(\"Highest 2-gram:\", word_frequency_2ngram.index[0])\n",
    "print(\"TF-IDF with 2-gram:\")\n",
    "print(\"Lowest 2-gram TF-IDF:\", tfidf_sort_2gram.index[-1])\n",
    "print(\"Highest 2-gram TF-IDF:\", tfidf_sort_2gram.index[0])\n",
    "print(\"TF-IDF with 2-gram (new hyperparameters):\")\n",
    "print(\"Lowest 2-gram TF-IDF:\", new_tfidf_sort_2gram.index[-1])\n",
    "print(\"Highest 2-gram TF-IDF:\", new_tfidf_sort_2gram.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. Which limitations from n-grams that TF-IDF overcame ?\n",
    "- TF-IDF can overcome stopwords problem of n-grams.\n",
    "- TF-IDF reduce weight of popular words.\n",
    "- TF-IDF separate important words from table, reduce size of useful data.\n",
    "- TF-IDF evaluate the importance of dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
