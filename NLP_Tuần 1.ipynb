{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "Vietnamese Students' Feedback Corpus (UIT-VSFC) is the resource consists of over 16,000 sentences which are human-annotated with two different tasks: sentiment-based and topic-based classifications.\n",
    "\n",
    "[1] Kiet Van Nguyen, Vu Duc Nguyen, Phu Xuan-Vinh Nguyen, Tham Thi-Hong Truong, Ngan Luu-Thuy Nguyen, UIT-VSFC: Vietnamese Students' Feedback Corpus for Sentiment Analysis,  2018 10th International Conference on Knowledge and Systems Engineering (KSE 2018), November 1-3, 2018, Ho Chi Minh City, Vietnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"uitnlp/vietnamese_students_feedback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentiment', 'topic'],\n",
       "    num_rows: 11426\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = dataset['train']\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'slide giáo trình đầy đủ .', 'sentiment': 2, 'topic': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11426"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slide giáo trình đầy đủ .'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a sentence\n",
    "example_word_list = train_set[0]['sentence']\n",
    "example_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slide', 'giáo', 'trình', 'đầy', 'đủ', '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split sentence word-by-word\n",
    "example_word_list.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slide giáo trình đầy đủ .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join words into 1 full sentence\n",
    "sentence = \"\"\n",
    "for word in example_word_list:\n",
    "    sentence += word\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slide giáo trình đầy đủ .',\n",
       " 'nhiệt tình giảng dạy , gần gũi với sinh viên .',\n",
       " 'đi học đầy đủ full điểm chuyên cần .',\n",
       " 'chưa áp dụng công nghệ thông tin và các thiết bị hỗ trợ cho việc giảng dạy .',\n",
       " 'thầy giảng bài hay , có nhiều bài tập ví dụ ngay trên lớp .',\n",
       " 'giảng viên đảm bảo thời gian lên lớp , tích cực trả lời câu hỏi của sinh viên , thường xuyên đặt câu hỏi cho sinh viên .',\n",
       " 'em sẽ nợ môn này , nhưng em sẽ học lại ở các học kỳ kế tiếp .',\n",
       " 'thời lượng học quá dài , không đảm bảo tiếp thu hiệu quả .',\n",
       " 'nội dung môn học có phần thiếu trọng tâm , hầu như là chung chung , khái quát khiến sinh viên rất khó nắm được nội dung môn học .',\n",
       " 'cần nói rõ hơn bằng cách trình bày lên bảng thay vì nhìn vào slide .']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 10 sentences to process\n",
    "sentence_list = []\n",
    "for idx in range(10):\n",
    "    sentence = \"\"\n",
    "    for word in train_set[idx]['sentence']:\n",
    "        sentence += word\n",
    "    sentence_list.append(sentence)\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "- N-grams are continuous sequences of words or symbols, or tokens in a document. In technical terms, they can be defined as the neighboring sequences of items in a document.\n",
    "- We can use n-grams or multiple other text preprocessing algorithms by incorporating [`nltk`](https://www.nltk.org/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: This document is the second document.\n",
      "===============\n",
      "1-gram: ['This', 'document', 'is', 'the', 'second', 'document.']\n",
      "\n",
      "2-gram: ['This document', 'document is', 'is the', 'the second', 'second document.']\n",
      "\n",
      "3-gram: ['This document is', 'document is the', 'is the second', 'the second document.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "\n",
    "num_of_grams = np.arange(1, 4, 1) # Test 3 n-grams\n",
    "\n",
    "print(\"Original sentence:\", example_sentence[1])\n",
    "print(\"===\"*5)\n",
    "\n",
    "for gram in num_of_grams:\n",
    "    splitted_sentence = ngrams(example_sentence[1].split(), int(gram))\n",
    "    print(f\"{gram}-gram: \",end ='')\n",
    "    n_grams_list = [' '.join(grams) for grams in splitted_sentence]\n",
    "    print(n_grams_list)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features with n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1  2  3\n",
       "and       0  0  1  0\n",
       "document  1  2  0  1\n",
       "first     1  0  0  1\n",
       "is        1  1  1  1\n",
       "one       0  0  1  0\n",
       "second    0  1  0  0\n",
       "the       1  1  1  1\n",
       "third     0  0  1  0\n",
       "this      1  1  1  1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 1))\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform(example_sentence).toarray()\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "word_frequency.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: giảng viên đảm bảo thời gian lên lớp , tích cực trả lời câu hỏi của sinh viên , thường xuyên đặt câu hỏi cho sinh viên .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bảo</th>\n",
       "      <th>cho</th>\n",
       "      <th>câu</th>\n",
       "      <th>của</th>\n",
       "      <th>cực</th>\n",
       "      <th>gian</th>\n",
       "      <th>giảng</th>\n",
       "      <th>hỏi</th>\n",
       "      <th>lên</th>\n",
       "      <th>lớp</th>\n",
       "      <th>lời</th>\n",
       "      <th>sinh</th>\n",
       "      <th>thường</th>\n",
       "      <th>thời</th>\n",
       "      <th>trả</th>\n",
       "      <th>tích</th>\n",
       "      <th>viên</th>\n",
       "      <th>xuyên</th>\n",
       "      <th>đảm</th>\n",
       "      <th>đặt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bảo  cho  câu  của  cực  gian  giảng  hỏi  lên  lớp  lời  sinh  thường  \\\n",
       "0    1    1    2    1    1     1      1    2    1    1    1     2       1   \n",
       "\n",
       "   thời  trả  tích  viên  xuyên  đảm  đặt  \n",
       "0     1    1     1     3      1    1    1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 1))\n",
    "\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform([sentence_list[5]]).toarray()\n",
    "\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "\n",
    "print('Example sentence:', sentence_list[5])\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and this</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document is</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first document</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is this</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this is</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this the</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0  1  2  3\n",
       "and              0  0  1  0\n",
       "and this         0  0  1  0\n",
       "document         1  2  0  1\n",
       "document is      0  1  0  0\n",
       "first            1  0  0  1\n",
       "first document   1  0  0  1\n",
       "is               1  1  1  1\n",
       "is the           1  1  1  0\n",
       "is this          0  0  0  1\n",
       "one              0  0  1  0\n",
       "second           0  1  0  0\n",
       "second document  0  1  0  0\n",
       "the              1  1  1  1\n",
       "the first        1  0  0  1\n",
       "the second       0  1  0  0\n",
       "the third        0  0  1  0\n",
       "third            0  0  1  0\n",
       "third one        0  0  1  0\n",
       "this             1  1  1  1\n",
       "this document    0  1  0  0\n",
       "this is          1  0  1  0\n",
       "this the         0  0  0  1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 2))\n",
    "\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform(example_sentence).toarray()\n",
    "\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "word_frequency.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 1\n",
    "Based on the UIT-VSFC dataset and the aforementioned information.\n",
    "- Create an $n$-gram word frequency table, such that $n$ could be any number of your desire.\n",
    "- With $n=1$ and $n=2$, what is the most popular word in the dataset ?\n",
    "- With $n=1$ and $n=2$, what is the rarest word in the dataset ?\n",
    "- What are the limitations of this data processing flow ? How can we overcome those ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve all sentences within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_all_sentences(dataset) -> List[str]:\n",
    "    \"\"\"\n",
    "    Function to get all sentences and store them into a list of strings\n",
    "\n",
    "    Args:\n",
    "    dataset -- The subset (i.e., train/valid/test) in UIT-VSFC dataset\n",
    "\n",
    "    Returns:\n",
    "    A list of all sentences in a subset data of the UIT-VSFC.\n",
    "    \"\"\"\n",
    "\n",
    "    list_all_sentence: list = []\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    for idx in range(len(dataset)):\n",
    "        sentence = \"\"\n",
    "        for word in dataset[idx]['sentence']:\n",
    "            sentence += word\n",
    "        list_all_sentence.append(sentence)\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return list_all_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences within the dataset: 11426\n",
      "Example sentence: nhiệt tình giảng dạy , gần gũi với sinh viên .\n"
     ]
    }
   ],
   "source": [
    "list_all_sentence: list = get_all_sentences(train_set)\n",
    "print(f\"#sentences within the dataset: {len(list_all_sentence)}\")\n",
    "print(f\"Example sentence: {list_all_sentence[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the word frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_word_frequency(sentence_list: list,\n",
    "                          n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to build a word frequency table based on n-grams\n",
    "\n",
    "    Args:\n",
    "    sentence_list (list) -- A list of all sentences needed for table constructing process\n",
    "    n (int) -- Number of grams that we parse into this function\n",
    "\n",
    "    Returns:\n",
    "    A dataframe contains all words after conducting n-grams and their respective frequencies\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    count_vectorize_model = CountVectorizer(ngram_range = (n, n))\n",
    "    n_grams_feature_vector = count_vectorize_model.fit_transform(sentence_list).toarray()\n",
    "    word_frequency_table = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return word_frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       10  100  10h  10h30  11  11doubledot55  11h30  11h55  12  \\\n",
      "0       0    0    0      0   0              0      0      0   0   \n",
      "1       0    0    0      0   0              0      0      0   0   \n",
      "2       0    0    0      0   0              0      0      0   0   \n",
      "3       0    0    0      0   0              0      0      0   0   \n",
      "4       0    0    0      0   0              0      0      0   0   \n",
      "...    ..  ...  ...    ...  ..            ...    ...    ...  ..   \n",
      "11421   0    0    0      0   0              0      0      0   0   \n",
      "11422   0    0    0      0   0              0      0      0   0   \n",
      "11423   0    0    0      0   0              0      0      0   0   \n",
      "11424   0    0    0      0   0              0      0      0   0   \n",
      "11425   0    0    0      0   0              0      0      0   0   \n",
      "\n",
      "       12doubledot00  ...  ấy  ẩn  ắt  ốc  ồn  ổn  ủa  ủng  ức  ứng  \n",
      "0                  0  ...   0   0   0   0   0   0   0    0   0    0  \n",
      "1                  0  ...   0   0   0   0   0   0   0    0   0    0  \n",
      "2                  0  ...   0   0   0   0   0   0   0    0   0    0  \n",
      "3                  0  ...   0   0   0   0   0   0   0    0   0    0  \n",
      "4                  0  ...   0   0   0   0   0   0   0    0   0    0  \n",
      "...              ...  ...  ..  ..  ..  ..  ..  ..  ..  ...  ..  ...  \n",
      "11421              0  ...   0   0   0   0   0   0   0    0   0    0  \n",
      "11422              0  ...   0   0   0   0   0   0   0    0   0    0  \n",
      "11423              0  ...   0   0   0   0   0   0   0    0   0    0  \n",
      "11424              0  ...   0   0   0   0   0   0   0    0   0    0  \n",
      "11425              0  ...   0   0   0   0   0   0   0    0   0    0  \n",
      "\n",
      "[11426 rows x 2459 columns]\n",
      "Từ phổ biến nhất: viên, Xuất hiện 4803 lần\n",
      "Từ hiếm gặp nhất: 11doubledot55, Xuất hiện 1 lần\n",
      "viên     4803\n",
      "giảng    3711\n",
      "dạy      3156\n",
      "thầy     3095\n",
      "sinh     3082\n",
      "học      2940\n",
      "bài      2336\n",
      "tình     2266\n",
      "không    2177\n",
      "và       2068\n",
      "dtype: int64\n",
      "dẽ               1\n",
      "english          1\n",
      "engine           1\n",
      "ốc               1\n",
      "3dsmax           1\n",
      "đặp              1\n",
      "đứt              1\n",
      "lấn              1\n",
      "lảng             1\n",
      "lạm              1\n",
      "dọa              1\n",
      "đếm              1\n",
      "đế               1\n",
      "gán              1\n",
      "ấm               1\n",
      "ướt              1\n",
      "ức               1\n",
      "đống             1\n",
      "đốn              1\n",
      "11doubledot55    1\n",
      "dtype: int64\n",
      "       10 50  10 bài  10 fraction  10 kiến  10 luôn  10 mấy  10 mới  10 người  \\\n",
      "0          0       0            0        0        0       0       0         0   \n",
      "1          0       0            0        0        0       0       0         0   \n",
      "2          0       0            0        0        0       0       0         0   \n",
      "3          0       0            0        0        0       0       0         0   \n",
      "4          0       0            0        0        0       0       0         0   \n",
      "...      ...     ...          ...      ...      ...     ...     ...       ...   \n",
      "11421      0       0            0        0        0       0       0         0   \n",
      "11422      0       0            0        0        0       0       0         0   \n",
      "11423      0       0            0        0        0       0       0         0   \n",
      "11424      0       0            0        0        0       0       0         0   \n",
      "11425      0       0            0        0        0       0       0         0   \n",
      "\n",
      "       10 năm  10 phút  ...  ứng kịp  ứng nhu  ứng nhưng  ứng tốt  ứng yêu  \\\n",
      "0           0        0  ...        0        0          0        0        0   \n",
      "1           0        0  ...        0        0          0        0        0   \n",
      "2           0        0  ...        0        0          0        0        0   \n",
      "3           0        0  ...        0        0          0        0        0   \n",
      "4           0        0  ...        0        0          0        0        0   \n",
      "...       ...      ...  ...      ...      ...        ...      ...      ...   \n",
      "11421       0        0  ...        0        0          0        0        0   \n",
      "11422       0        0  ...        0        0          0        0        0   \n",
      "11423       0        0  ...        0        0          0        0        0   \n",
      "11424       0        0  ...        0        0          0        0        0   \n",
      "11425       0        0  ...        0        0          0        0        0   \n",
      "\n",
      "       ứng đáp  ứng đúng  ứng được  ứng đầy  ứng đủ  \n",
      "0            0         0         0        0       0  \n",
      "1            0         0         0        0       0  \n",
      "2            0         0         0        0       0  \n",
      "3            0         0         0        0       0  \n",
      "4            0         0         0        0       0  \n",
      "...        ...       ...       ...      ...     ...  \n",
      "11421        0         0         0        0       0  \n",
      "11422        0         0         0        0       0  \n",
      "11423        0         0         0        0       0  \n",
      "11424        0         0         0        0       0  \n",
      "11425        0         0         0        0       0  \n",
      "\n",
      "[11426 rows x 31384 columns]\n",
      "Từ phổ biến nhất: sinh viên, Xuất hiện 2698 lần\n",
      "Từ hiếm gặp nhất: trên google, Xuất hiện 1 lần\n",
      "sinh viên     2698\n",
      "nhiệt tình    1848\n",
      "giảng viên    1610\n",
      "bài tập       1057\n",
      "dễ hiểu       1004\n",
      "giảng dạy      956\n",
      "kiến thức      904\n",
      "thực hành      877\n",
      "môn học        688\n",
      "cho sinh       656\n",
      "dtype: int64\n",
      "ứng đúng            1\n",
      "học dồn             1\n",
      "ứng đầy             1\n",
      "ổn thỏa             1\n",
      "học giải            1\n",
      "10 phút             1\n",
      "10 sinh             1\n",
      "10 thì              1\n",
      "10 trong            1\n",
      "học grammar         1\n",
      "100 cách            1\n",
      "100 là              1\n",
      "100 người           1\n",
      "100 tự              1\n",
      "10h mới             1\n",
      "10h30 nhưng         1\n",
      "11 thì              1\n",
      "11doubledot55 pm    1\n",
      "11h30 nghỉ          1\n",
      "trên google         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Construct the table of word frequency\n",
    "# 1-ngram\n",
    "word_frequency_table = n_gram_word_frequency(sentence_list=list_all_sentence,\n",
    "                                             n=1)\n",
    "print(word_frequency_table)\n",
    "\n",
    "# In ra từ phổ biến nhất và hiếm gặp nhất trong tất cả các câu\n",
    "sum_word_frequency_table = word_frequency_table.sum(axis=0).sort_values(ascending=False)\n",
    "print(f\"Từ phổ biến nhất: {sum_word_frequency_table.index[0]}, Xuất hiện {sum_word_frequency_table.iloc[0]} lần\")\n",
    "print(f\"Từ hiếm gặp nhất: {sum_word_frequency_table.index[-1]}, Xuất hiện {sum_word_frequency_table.iloc[-1]} lần\")\n",
    "\n",
    "print(sum_word_frequency_table[:10])\n",
    "print(sum_word_frequency_table[-10:])\n",
    "\n",
    "# 2-ngram\n",
    "word_frequency_table = n_gram_word_frequency(sentence_list=list_all_sentence,\n",
    "                                             n=2)\n",
    "print(word_frequency_table)\n",
    "\n",
    "# In ra từ phổ biến nhất và hiếm gặp nhất trong tất cả các câu\n",
    "sum_word_frequency_table = word_frequency_table.sum(axis=0).sort_values(ascending=False)\n",
    "print(f\"Từ phổ biến nhất: {sum_word_frequency_table.index[0]}, Xuất hiện {sum_word_frequency_table.iloc[0]} lần\")\n",
    "print(f\"Từ hiếm gặp nhất: {sum_word_frequency_table.index[-1]}, Xuất hiện {sum_word_frequency_table.iloc[-1]} lần\")\n",
    "\n",
    "print(sum_word_frequency_table[:10])\n",
    "print(sum_word_frequency_table[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the limitations of this data processing flow ? How can we overcome those ?\n",
    "# 1. The dataset is not large enough to train a good model\n",
    "# 2. The dataset is not balanced enough to train a good model\n",
    "# 3. The dataset is not diverse enough to train a good model\n",
    "# 4. The dataset is not clean enough to train a good model\n",
    "# 5. The dataset is not representative enough to train a good model\n",
    "# 6. The dataset is not annotated enough to train a good model\n",
    "# 7. The dataset is not labeled enough to train a good model\n",
    "# 8. The dataset is not structured enough to train a good model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should comment your answer to problem 1 here with sufficient explanations, including your implementation and reasoning.\n",
    "\n",
    "- Stopwords gây nhiễu kết quả\n",
    "- Không thể phân biệt từ đồng nghĩa\n",
    "- Không xử lý lỗi chính tả\n",
    "- Không phân biệt ngữ cảnh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloaded file: vietnamese-stopwords (1).txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the stopword dictionary\n",
    "import wget\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt\"\n",
    "filename = wget.download(url)\n",
    "print(f\"\\nDownloaded file: {filename}\")\n",
    "\n",
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Number of stop words: 1942\n"
     ]
    }
   ],
   "source": [
    "# Observe stopwords list\n",
    "vietnamese_stopword = open('vietnamese-stopwords.txt', 'r', encoding='utf-8').read()\n",
    "vietnamese_stopword = vietnamese_stopword.split('\\n') # Separate lines by lines\n",
    "print(f\"#Number of stop words: {len(vietnamese_stopword)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a lô\n",
      "a ha\n",
      "ai\n",
      "ai ai\n",
      "ai nấy\n",
      "ai đó\n",
      "alô\n",
      "amen\n",
      "anh\n",
      "anh ấy\n"
     ]
    }
   ],
   "source": [
    "# Stop words example\n",
    "for sentence in vietnamese_stopword[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency - Invert document frequency (TF-IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF\n",
    "Term frequency (TF) is the number of times a given term appears in document\n",
    "\n",
    "$$\n",
    "tf(t) = f(t,d)\\times\\frac{1}{T}\n",
    "$$\n",
    "whereas, $f(t,d)$ is the frequency of the word $t$ in the document $d$, $T$ is the number of all words in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf\n",
       "and       0.0\n",
       "document  0.2\n",
       "first     0.2\n",
       "is        0.2\n",
       "one       0.0\n",
       "second    0.0\n",
       "the       0.2\n",
       "third     0.0\n",
       "this      0.2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Declare TF vectorize\n",
    "tf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                use_idf=False, # only using TF\n",
    "                                norm='l1')\n",
    "\n",
    "tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf_vectorized = tf_vectorizer.transform(corpus)\n",
    "\n",
    "tf_output = tf_vectorized[0]\n",
    "\n",
    "# Build TF table\n",
    "words_tf_idf = pd.DataFrame(tf_output.T.todense(), index=tf_vectorizer.get_feature_names_out(), columns=['tf'])\n",
    "words_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF\n",
    "\n",
    "Inverse Document Frequency, or abbreviated as IDF, measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones.\n",
    "\n",
    "$$\n",
    "idf(t) = \\log\\left(\\frac{\\text{#documents in the document set}}{\\text{#documents with term}}\\right) + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf       idf\n",
       "is        0.2  1.000000\n",
       "the       0.2  1.000000\n",
       "this      0.2  1.000000\n",
       "document  0.2  1.287682\n",
       "first     0.2  1.693147\n",
       "and       0.0  2.386294\n",
       "second    0.0  2.386294\n",
       "one       0.0  2.386294\n",
       "third     0.0  2.386294"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Configure settings for IDF vectorize\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                    smooth_idf=False,\n",
    "                                    use_idf=True,\n",
    "                                    norm=None)\n",
    "\n",
    "tf_idf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Retrieve only idf information\n",
    "idf_vectorizer = tf_idf_vectorizer.idf_\n",
    "\n",
    "# Join idf values into the previous dataframe\n",
    "words_tf_idf['idf'] = idf_vectorizer\n",
    "\n",
    "# Show dataframe with ascending values of idf\n",
    "words_tf_idf.sort_values(by=['idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Technically saying, TF-IDF is a score which is applied to every word in every document in our dataset. And for every word, the TF-IDF value increases with every appearance of the word in a document, but is gradually decreased with every appearance in other documents\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}= tf(t, d) \\times idf(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.215302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.283096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf       idf    tf-idf\n",
       "and       0.0  2.386294  0.000000\n",
       "third     0.0  2.386294  0.000000\n",
       "second    0.0  2.386294  0.000000\n",
       "one       0.0  2.386294  0.000000\n",
       "is        0.2  1.000000  0.167201\n",
       "the       0.2  1.000000  0.167201\n",
       "this      0.2  1.000000  0.167201\n",
       "document  0.2  1.287682  0.215302\n",
       "first     0.2  1.693147  0.283096"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                    smooth_idf=False,\n",
    "                                    use_idf=True,\n",
    "                                    norm='l1')\n",
    "\n",
    "tf_idf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf_idf_vectorized = tf_idf_vectorizer.transform(corpus)\n",
    "\n",
    "tf_idf_output = tf_idf_vectorized[0]\n",
    "words_tf_idf['tf-idf'] = tf_idf_output.T.todense()\n",
    "\n",
    "words_tf_idf.sort_values(by=['tf-idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem set 2\n",
    "Based on the problem 1 and the instruction on TF, IDF, TF-IDF:\n",
    "- (2a) Build the tf-idf table for the UIT-VSFC dataset with $n$-gram = 1 and $n$-gram = 2.\n",
    "- (2b) Change a few hyperparameters in the `TfidfVectorizer` function (`smooth_idf`, `sublinear_tf` and `norm`) from problem 2a (*you could browse from this [link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to discover which are the correct paramters to parse*). Explain the results differences collected after modifying hyperparameters.\n",
    "- (2c) Which words has the lowest and the highest tf-idf values ? Do they differ from $n$-grams results ?\n",
    "- (2d) Which limitations from $n$-grams that TF-IDF overcame ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 1-gram:\n",
      "        10  100  10h  10h30   11  11doubledot55  11h30  11h55   12  \\\n",
      "0      0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "1      0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "2      0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "3      0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "4      0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "...    ...  ...  ...    ...  ...            ...    ...    ...  ...   \n",
      "11421  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "11422  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "11423  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "11424  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "11425  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "\n",
      "       12doubledot00  ...   ấy   ẩn   ắt   ốc   ồn   ổn   ủa  ủng   ức  ứng  \n",
      "0                0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1                0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2                0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3                0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4                0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...              ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "11421            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "11422            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "11423            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "11424            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "11425            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[11426 rows x 2459 columns]\n",
      "\n",
      "TF-IDF with 2-gram:\n",
      "        10  10 50  10 bài  10 fraction  10 kiến  10 luôn  10 mấy  10 mới  \\\n",
      "0      0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "1      0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "2      0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "3      0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "4      0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "...    ...    ...     ...          ...      ...      ...     ...     ...   \n",
      "11421  0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "11422  0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "11423  0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "11424  0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "11425  0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "\n",
      "       10 người  10 năm  ...  ứng kịp  ứng nhu  ứng nhưng  ứng tốt  ứng yêu  \\\n",
      "0           0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "1           0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "2           0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "3           0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "4           0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "...         ...     ...  ...      ...      ...        ...      ...      ...   \n",
      "11421       0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "11422       0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "11423       0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "11424       0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "11425       0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "\n",
      "       ứng đáp  ứng đúng  ứng được  ứng đầy  ứng đủ  \n",
      "0          0.0       0.0       0.0      0.0     0.0  \n",
      "1          0.0       0.0       0.0      0.0     0.0  \n",
      "2          0.0       0.0       0.0      0.0     0.0  \n",
      "3          0.0       0.0       0.0      0.0     0.0  \n",
      "4          0.0       0.0       0.0      0.0     0.0  \n",
      "...        ...       ...       ...      ...     ...  \n",
      "11421      0.0       0.0       0.0      0.0     0.0  \n",
      "11422      0.0       0.0       0.0      0.0     0.0  \n",
      "11423      0.0       0.0       0.0      0.0     0.0  \n",
      "11424      0.0       0.0       0.0      0.0     0.0  \n",
      "11425      0.0       0.0       0.0      0.0     0.0  \n",
      "\n",
      "[11426 rows x 33843 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# (2a) Build the tf-idf table for the UIT-VSFC dataset with $n$-gram = 1 and $n$-gram = 2\n",
    "tfidf_vectorizer_1gram = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf_vectorizer_2gram = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "tfidf_1gram = tfidf_vectorizer_1gram.fit_transform(list_all_sentence)\n",
    "tfidf_2gram = tfidf_vectorizer_2gram.fit_transform(list_all_sentence)\n",
    "\n",
    "tfidf_1gram_df = pd.DataFrame(tfidf_1gram.toarray(), columns=tfidf_vectorizer_1gram.get_feature_names_out())\n",
    "tfidf_2gram_df = pd.DataFrame(tfidf_2gram.toarray(), columns=tfidf_vectorizer_2gram.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF with 1-gram:\")\n",
    "print(tfidf_1gram_df)\n",
    "print(\"\\nTF-IDF with 2-gram:\")\n",
    "print(tfidf_2gram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 1-gram (sublinear_tf=True, smooth_idf=False, norm='l2'):\n",
      "        10  100  10h  10h30   11  11doubledot55  11h30  11h55   12  \\\n",
      "0      0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "1      0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "2      0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "3      0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "4      0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "...    ...  ...  ...    ...  ...            ...    ...    ...  ...   \n",
      "11421  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "11422  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "11423  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "11424  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "11425  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0   \n",
      "\n",
      "       12doubledot00  ...   ấy   ẩn   ắt   ốc   ồn   ổn   ủa  ủng   ức  ứng  \n",
      "0                0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1                0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2                0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3                0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4                0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...              ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "11421            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "11422            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "11423            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "11424            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "11425            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[11426 rows x 2459 columns]\n",
      "\n",
      "TF-IDF with 2-gram (sublinear_tf=True, smooth_idf=False, norm='l2'):\n",
      "        10  10 50  10 bài  10 fraction  10 kiến  10 luôn  10 mấy  10 mới  \\\n",
      "0      0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "1      0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "2      0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "3      0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "4      0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "...    ...    ...     ...          ...      ...      ...     ...     ...   \n",
      "11421  0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "11422  0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "11423  0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "11424  0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "11425  0.0    0.0     0.0          0.0      0.0      0.0     0.0     0.0   \n",
      "\n",
      "       10 người  10 năm  ...  ứng kịp  ứng nhu  ứng nhưng  ứng tốt  ứng yêu  \\\n",
      "0           0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "1           0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "2           0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "3           0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "4           0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "...         ...     ...  ...      ...      ...        ...      ...      ...   \n",
      "11421       0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "11422       0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "11423       0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "11424       0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "11425       0.0     0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "\n",
      "       ứng đáp  ứng đúng  ứng được  ứng đầy  ứng đủ  \n",
      "0          0.0       0.0       0.0      0.0     0.0  \n",
      "1          0.0       0.0       0.0      0.0     0.0  \n",
      "2          0.0       0.0       0.0      0.0     0.0  \n",
      "3          0.0       0.0       0.0      0.0     0.0  \n",
      "4          0.0       0.0       0.0      0.0     0.0  \n",
      "...        ...       ...       ...      ...     ...  \n",
      "11421      0.0       0.0       0.0      0.0     0.0  \n",
      "11422      0.0       0.0       0.0      0.0     0.0  \n",
      "11423      0.0       0.0       0.0      0.0     0.0  \n",
      "11424      0.0       0.0       0.0      0.0     0.0  \n",
      "11425      0.0       0.0       0.0      0.0     0.0  \n",
      "\n",
      "[11426 rows x 33843 columns]\n"
     ]
    }
   ],
   "source": [
    "# (2b) Change a few hyperparameters in the TfidfVectorizer function\n",
    "# 1-ngram\n",
    "tfidf_vectorizer_1gram = TfidfVectorizer(ngram_range=(1, 1), smooth_idf=False, sublinear_tf=True, norm='l1')\n",
    "tfidf_1gram = tfidf_vectorizer_1gram.fit_transform(list_all_sentence)\n",
    "tfidf_1gram_df = pd.DataFrame(tfidf_1gram.toarray(), columns=tfidf_vectorizer_1gram.get_feature_names_out())\n",
    "print(\"TF-IDF with 1-gram (sublinear_tf=True, smooth_idf=False, norm='l2'):\")\n",
    "print(tfidf_1gram_df)\n",
    "# 2-ngram\n",
    "tfidf_vectorizer_2gram = TfidfVectorizer(ngram_range=(1, 2), smooth_idf=False, sublinear_tf=True, norm='l1')\n",
    "tfidf_2gram = tfidf_vectorizer_2gram.fit_transform(list_all_sentence)\n",
    "tfidf_2gram_df = pd.DataFrame(tfidf_2gram.toarray(), columns=tfidf_vectorizer_2gram.get_feature_names_out())\n",
    "print(\"\\nTF-IDF with 2-gram (sublinear_tf=True, smooth_idf=False, norm='l2'):\")\n",
    "print(tfidf_2gram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest 1-gram TF-IDF values:\n",
      "ấm     0.0\n",
      "ảo     0.0\n",
      "ảnh    0.0\n",
      "ướt    0.0\n",
      "ước    0.0\n",
      "      ... \n",
      "ủa     0.0\n",
      "ủng    0.0\n",
      "ức     0.0\n",
      "ứng    0.0\n",
      "10     0.0\n",
      "Length: 2459, dtype: float64\n",
      "\n",
      "Highest 1-gram TF-IDF values:\n",
      "dễ       1.000000\n",
      "chán     1.000000\n",
      "không    1.000000\n",
      "xinh     1.000000\n",
      "hết      1.000000\n",
      "           ...   \n",
      "tá       0.023056\n",
      "lõng     0.016674\n",
      "ùn       0.016674\n",
      "vạ       0.016674\n",
      "gạo      0.016674\n",
      "Length: 2459, dtype: float64\n",
      "\n",
      "Lowest 2-gram TF-IDF values:\n",
      "ứng đúng     0.0\n",
      "ứng đáp      0.0\n",
      "ứng yêu      0.0\n",
      "ứng tốt      0.0\n",
      "ứng nhưng    0.0\n",
      "            ... \n",
      "100 là       0.0\n",
      "100 cách     0.0\n",
      "ứng đầy      0.0\n",
      "ứng đủ       0.0\n",
      "10           0.0\n",
      "Length: 33843, dtype: float64\n",
      "\n",
      "Highest 2-gram TF-IDF values:\n",
      "tệ            1.000000\n",
      "tốt           1.000000\n",
      "giỏi          1.000000\n",
      "everything    1.000000\n",
      "dễ            1.000000\n",
      "                ...   \n",
      "gạo một       0.005771\n",
      "tới bị        0.005771\n",
      "lõng          0.005771\n",
      "lõng mơ       0.005771\n",
      "cả tên        0.005771\n",
      "Length: 33843, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# (2c) Find words with the lowest and highest tf-idf values\n",
    "# 1-ngram\n",
    "lowest_1gram = tfidf_1gram_df.min(axis=0).sort_values(ascending=True)\n",
    "highest_1gram = tfidf_1gram_df.max(axis=0).sort_values(ascending=False)\n",
    "print(\"Lowest 1-gram TF-IDF values:\")\n",
    "print(lowest_1gram)\n",
    "print(\"\\nHighest 1-gram TF-IDF values:\")\n",
    "print(highest_1gram)\n",
    "# 2-ngram\n",
    "lowest_2gram = tfidf_2gram_df.min(axis=0).sort_values(ascending=True)\n",
    "highest_2gram = tfidf_2gram_df.max(axis=0).sort_values(ascending=False)\n",
    "print(\"\\nLowest 2-gram TF-IDF values:\")\n",
    "print(lowest_2gram)\n",
    "print(\"\\nHighest 2-gram TF-IDF values:\")\n",
    "print(highest_2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2d) Which limitations from $n$-grams that TF-IDF overcame ?\n",
    "# TF-IDF overcame the limitations of n\n",
    "# -grams by considering the importance of each word in the context of the entire document.\n",
    "# It assigns higher weights to words that are more informative and less frequent across the entire corpus, thus reducing the impact of common words that may not carry significant meaning.\n",
    "# TF-IDF also helps to mitigate the issue of high-dimensionality associated with n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF với n=1 (Unigram):\n",
      "    10  100  10h  10h30   11  11doubledot55  11h30  11h55   12  12doubledot00  \\\n",
      "0  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
      "1  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
      "2  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
      "3  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
      "4  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
      "\n",
      "   ...   ấy   ẩn   ắt   ốc   ồn   ổn   ủa  ủng   ức  ứng  \n",
      "0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 2459 columns]\n",
      "\n",
      "TF-IDF với n=2 (Bigram):\n",
      "   10 50  10 bài  10 fraction  10 kiến  10 luôn  10 mấy  10 mới  10 người  \\\n",
      "0    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
      "1    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
      "2    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
      "3    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
      "4    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
      "\n",
      "   10 năm  10 phút  ...  ứng kịp  ứng nhu  ứng nhưng  ứng tốt  ứng yêu  \\\n",
      "0     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "1     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "2     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "3     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "4     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
      "\n",
      "   ứng đáp  ứng đúng  ứng được  ứng đầy  ứng đủ  \n",
      "0      0.0       0.0       0.0      0.0     0.0  \n",
      "1      0.0       0.0       0.0      0.0     0.0  \n",
      "2      0.0       0.0       0.0      0.0     0.0  \n",
      "3      0.0       0.0       0.0      0.0     0.0  \n",
      "4      0.0       0.0       0.0      0.0     0.0  \n",
      "\n",
      "[5 rows x 31384 columns]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Tải dữ liệu UIT-VSFC\n",
    "dataset = load_dataset(\"uitnlp/vietnamese_students_feedback\")\n",
    "sentences = [data[\"sentence\"].lower() for data in dataset[\"train\"]]  # Chuyển về chữ thường\n",
    "\n",
    "# Hàm tính TF-IDF\n",
    "def compute_tfidf(sentences, ngram_range):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range)  # n-gram (1,1) hoặc (2,2)\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)  # Ma trận TF-IDF\n",
    "    feature_names = vectorizer.get_feature_names_out()  # Danh sách từ/ngram\n",
    "    return pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)  # Chuyển thành DataFrame\n",
    "\n",
    "# Tạo bảng TF-IDF cho n=1 (unigram) và n=2 (bigram)\n",
    "tfidf_unigram = compute_tfidf(sentences, (1,1))\n",
    "tfidf_bigram = compute_tfidf(sentences, (2,2))\n",
    "\n",
    "# Hiển thị một phần của bảng TF-IDF\n",
    "print(\"TF-IDF với n=1 (Unigram):\")\n",
    "print(tfidf_unigram.head())\n",
    "\n",
    "print(\"\\nTF-IDF với n=2 (Bigram):\")\n",
    "print(tfidf_bigram.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF sau khi thay đổi tham số:\n",
      "    10  100  10h  10h30   11  11doubledot55  11h30  11h55   12  12doubledot00  \\\n",
      "0  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
      "1  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
      "2  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
      "3  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
      "4  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
      "\n",
      "   ...   ấy   ẩn   ắt   ốc   ồn   ổn   ủa  ủng   ức  ứng  \n",
      "0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 2459 columns]\n"
     ]
    }
   ],
   "source": [
    "# Thử nghiệm thay đổi siêu tham số\n",
    "vectorizer_modified = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, sublinear_tf=True, norm=\"l2\")\n",
    "tfidf_matrix_modified = vectorizer_modified.fit_transform(sentences)\n",
    "feature_names_modified = vectorizer_modified.get_feature_names_out()\n",
    "\n",
    "# Chuyển kết quả thành DataFrame\n",
    "tfidf_df_modified = pd.DataFrame(tfidf_matrix_modified.toarray(), columns=feature_names_modified)\n",
    "\n",
    "print(\"TF-IDF sau khi thay đổi tham số:\")\n",
    "print(tfidf_df_modified.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Từ có TF-IDF cao nhất (Unigram): ('viên', np.float64(659.3241372203528))\n",
      "Từ có TF-IDF thấp nhất (Unigram): ('vạ', np.float64(0.120081257215534))\n",
      "\n",
      "Từ có TF-IDF cao nhất (Bigram): ('nhiệt tình', np.float64(327.39125586167046))\n",
      "Từ có TF-IDF thấp nhất (Bigram): ('lạc lõng', np.float64(0.0952086904461525))\n"
     ]
    }
   ],
   "source": [
    "# Tính tổng giá trị TF-IDF cho từng từ/ngram\n",
    "unigram_tfidf_sum = tfidf_unigram.sum().sort_values(ascending=False)\n",
    "bigram_tfidf_sum = tfidf_bigram.sum().sort_values(ascending=False)\n",
    "\n",
    "# Tìm từ có giá trị TF-IDF cao nhất và thấp nhất\n",
    "most_important_unigram = unigram_tfidf_sum.idxmax(), unigram_tfidf_sum.max()\n",
    "least_important_unigram = unigram_tfidf_sum.idxmin(), unigram_tfidf_sum.min()\n",
    "\n",
    "most_important_bigram = bigram_tfidf_sum.idxmax(), bigram_tfidf_sum.max()\n",
    "least_important_bigram = bigram_tfidf_sum.idxmin(), bigram_tfidf_sum.min()\n",
    "\n",
    "print(\"Từ có TF-IDF cao nhất (Unigram):\", most_important_unigram)\n",
    "print(\"Từ có TF-IDF thấp nhất (Unigram):\", least_important_unigram)\n",
    "\n",
    "print(\"\\nTừ có TF-IDF cao nhất (Bigram):\", most_important_bigram)\n",
    "print(\"Từ có TF-IDF thấp nhất (Bigram):\", least_important_bigram)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
