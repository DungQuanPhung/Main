{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "Vietnamese Students' Feedback Corpus (UIT-VSFC) is the resource consists of over 16,000 sentences which are human-annotated with two different tasks: sentiment-based and topic-based classifications.\n",
    "\n",
    "[1] Kiet Van Nguyen, Vu Duc Nguyen, Phu Xuan-Vinh Nguyen, Tham Thi-Hong Truong, Ngan Luu-Thuy Nguyen, UIT-VSFC: Vietnamese Students' Feedback Corpus for Sentiment Analysis,  2018 10th International Conference on Knowledge and Systems Engineering (KSE 2018), November 1-3, 2018, Ho Chi Minh City, Vietnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"uitnlp/vietnamese_students_feedback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentiment', 'topic'],\n",
       "    num_rows: 11426\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = dataset['train']\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'slide giáo trình đầy đủ .', 'sentiment': 2, 'topic': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11426"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slide giáo trình đầy đủ .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a sentence\n",
    "example_word_list = train_set[0]['sentence']\n",
    "example_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slide', 'giáo', 'trình', 'đầy', 'đủ', '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split sentence word-by-word\n",
    "example_word_list.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slide giáo trình đầy đủ .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join words into 1 full sentence\n",
    "sentence = \"\"\n",
    "for word in example_word_list:\n",
    "    sentence += word\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slide giáo trình đầy đủ .',\n",
       " 'nhiệt tình giảng dạy , gần gũi với sinh viên .',\n",
       " 'đi học đầy đủ full điểm chuyên cần .',\n",
       " 'chưa áp dụng công nghệ thông tin và các thiết bị hỗ trợ cho việc giảng dạy .',\n",
       " 'thầy giảng bài hay , có nhiều bài tập ví dụ ngay trên lớp .',\n",
       " 'giảng viên đảm bảo thời gian lên lớp , tích cực trả lời câu hỏi của sinh viên , thường xuyên đặt câu hỏi cho sinh viên .',\n",
       " 'em sẽ nợ môn này , nhưng em sẽ học lại ở các học kỳ kế tiếp .',\n",
       " 'thời lượng học quá dài , không đảm bảo tiếp thu hiệu quả .',\n",
       " 'nội dung môn học có phần thiếu trọng tâm , hầu như là chung chung , khái quát khiến sinh viên rất khó nắm được nội dung môn học .',\n",
       " 'cần nói rõ hơn bằng cách trình bày lên bảng thay vì nhìn vào slide .']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 10 sentences to process\n",
    "sentence_list = []\n",
    "for idx in range(10):\n",
    "    sentence = \"\"\n",
    "    for word in train_set[idx]['sentence']:\n",
    "        sentence += word\n",
    "    sentence_list.append(sentence)\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "- N-grams are continuous sequences of words or symbols, or tokens in a document. In technical terms, they can be defined as the neighboring sequences of items in a document.\n",
    "- We can use n-grams or multiple other text preprocessing algorithms by incorporating [`nltk`](https://www.nltk.org/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: This document is the second document.\n",
      "===============\n",
      "1-gram: ['This', 'document', 'is', 'the', 'second', 'document.']\n",
      "\n",
      "2-gram: ['This document', 'document is', 'is the', 'the second', 'second document.']\n",
      "\n",
      "3-gram: ['This document is', 'document is the', 'is the second', 'the second document.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "\n",
    "num_of_grams = np.arange(1, 4, 1) # Test 3 n-grams\n",
    "\n",
    "print(\"Original sentence:\", example_sentence[1])\n",
    "print(\"===\"*5)\n",
    "\n",
    "for gram in num_of_grams:\n",
    "    splitted_sentence = ngrams(example_sentence[1].split(), int(gram))\n",
    "    print(f\"{gram}-gram: \",end ='')\n",
    "    n_grams_list = [' '.join(grams) for grams in splitted_sentence]\n",
    "    print(n_grams_list)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features with n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1  2  3\n",
       "and       0  0  1  0\n",
       "document  1  2  0  1\n",
       "first     1  0  0  1\n",
       "is        1  1  1  1\n",
       "one       0  0  1  0\n",
       "second    0  1  0  0\n",
       "the       1  1  1  1\n",
       "third     0  0  1  0\n",
       "this      1  1  1  1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 1))\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform(example_sentence).toarray()\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "word_frequency.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: giảng viên đảm bảo thời gian lên lớp , tích cực trả lời câu hỏi của sinh viên , thường xuyên đặt câu hỏi cho sinh viên .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bảo</th>\n",
       "      <th>cho</th>\n",
       "      <th>câu</th>\n",
       "      <th>của</th>\n",
       "      <th>cực</th>\n",
       "      <th>gian</th>\n",
       "      <th>giảng</th>\n",
       "      <th>hỏi</th>\n",
       "      <th>lên</th>\n",
       "      <th>lớp</th>\n",
       "      <th>lời</th>\n",
       "      <th>sinh</th>\n",
       "      <th>thường</th>\n",
       "      <th>thời</th>\n",
       "      <th>trả</th>\n",
       "      <th>tích</th>\n",
       "      <th>viên</th>\n",
       "      <th>xuyên</th>\n",
       "      <th>đảm</th>\n",
       "      <th>đặt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bảo  cho  câu  của  cực  gian  giảng  hỏi  lên  lớp  lời  sinh  thường  \\\n",
       "0    1    1    2    1    1     1      1    2    1    1    1     2       1   \n",
       "\n",
       "   thời  trả  tích  viên  xuyên  đảm  đặt  \n",
       "0     1    1     1     3      1    1    1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 1))\n",
    "\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform([sentence_list[5]]).toarray()\n",
    "\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "\n",
    "print('Example sentence:', sentence_list[5])\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and this</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document is</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first document</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is this</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this is</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this the</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0  1  2  3\n",
       "and              0  0  1  0\n",
       "and this         0  0  1  0\n",
       "document         1  2  0  1\n",
       "document is      0  1  0  0\n",
       "first            1  0  0  1\n",
       "first document   1  0  0  1\n",
       "is               1  1  1  1\n",
       "is the           1  1  1  0\n",
       "is this          0  0  0  1\n",
       "one              0  0  1  0\n",
       "second           0  1  0  0\n",
       "second document  0  1  0  0\n",
       "the              1  1  1  1\n",
       "the first        1  0  0  1\n",
       "the second       0  1  0  0\n",
       "the third        0  0  1  0\n",
       "third            0  0  1  0\n",
       "third one        0  0  1  0\n",
       "this             1  1  1  1\n",
       "this document    0  1  0  0\n",
       "this is          1  0  1  0\n",
       "this the         0  0  0  1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 2))\n",
    "\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform(example_sentence).toarray()\n",
    "\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "word_frequency.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 1\n",
    "Based on the UIT-VSFC dataset and the aforementioned information.\n",
    "- Create an $n$-gram word frequency table, such that $n$ could be any number of your desire.\n",
    "- With $n=1$ and $n=2$, what is the most popular word in the dataset ?\n",
    "- With $n=1$ and $n=2$, what is the rarest word in the dataset ?\n",
    "- What are the limitations of this data processing flow ? How can we overcome those ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve all sentences within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_all_sentences(dataset) -> List[str]:\n",
    "    \"\"\"\n",
    "    Function to get all sentences and store them into a list of strings\n",
    "\n",
    "    Args:\n",
    "    dataset -- The subset (i.e., train/valid/test) in UIT-VSFC dataset\n",
    "\n",
    "    Returns:\n",
    "    A list of all sentences in a subset data of the UIT-VSFC.\n",
    "    \"\"\"\n",
    "\n",
    "    list_all_sentence: list = []\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    for idx in range(len(train_set)):\n",
    "        sentence = \"\"\n",
    "        for word in example_word_list:\n",
    "            sentence += word\n",
    "        list_all_sentence.append(sentence)\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return list_all_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences within the dataset: 11426\n",
      "Example sentence: slide giáo trình đầy đủ .\n"
     ]
    }
   ],
   "source": [
    "list_all_sentence: list = get_all_sentences(train_set)\n",
    "print(f\"#sentences within the dataset: {len(list_all_sentence)}\")\n",
    "print(f\"Example sentence: {list_all_sentence[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the word frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_word_frequency(sentence_list: list,\n",
    "                          n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to build a word frequency table based on n-grams\n",
    "\n",
    "    Args:\n",
    "    sentence_list (list) -- A list of all sentences needed for table constructing process\n",
    "    n (int) -- Number of grams that we parse into this function\n",
    "\n",
    "    Returns:\n",
    "    A dataframe contains all words after conducting n-grams and their respective frequencies\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    count_vectorize_model = CountVectorizer(ngram_range = (n, n))\n",
    "    n_grams_feature_vector = count_vectorize_model.fit_transform(sentence_list).toarray()\n",
    "    word_frequency_table = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return word_frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>giáo trình đầy</th>\n",
       "      <th>slide giáo trình</th>\n",
       "      <th>trình đầy đủ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11421</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11422</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11423</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11424</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11425</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11426 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       giáo trình đầy  slide giáo trình  trình đầy đủ\n",
       "0                   1                 1             1\n",
       "1                   1                 1             1\n",
       "2                   1                 1             1\n",
       "3                   1                 1             1\n",
       "4                   1                 1             1\n",
       "...               ...               ...           ...\n",
       "11421               1                 1             1\n",
       "11422               1                 1             1\n",
       "11423               1                 1             1\n",
       "11424               1                 1             1\n",
       "11425               1                 1             1\n",
       "\n",
       "[11426 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the table of word frequency\n",
    "word_frequency_table = n_gram_word_frequency(sentence_list=list_all_sentence,\n",
    "                                             n=3)\n",
    "word_frequency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should comment your answer to problem 1 here with sufficient explanations, including your implementation and reasoning.\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloaded file: vietnamese-stopwords (1).txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the stopword dictionary\n",
    "import wget\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt\"\n",
    "filename = wget.download(url)\n",
    "print(f\"\\nDownloaded file: {filename}\")\n",
    "\n",
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Number of stop words: 1942\n"
     ]
    }
   ],
   "source": [
    "# Observe stopwords list\n",
    "vietnamese_stopword = open('vietnamese-stopwords.txt', 'r', encoding='utf-8').read()\n",
    "vietnamese_stopword = vietnamese_stopword.split('\\n') # Separate lines by lines\n",
    "print(f\"#Number of stop words: {len(vietnamese_stopword)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a lô\n",
      "a ha\n",
      "ai\n",
      "ai ai\n",
      "ai nấy\n",
      "ai đó\n",
      "alô\n",
      "amen\n",
      "anh\n",
      "anh ấy\n"
     ]
    }
   ],
   "source": [
    "# Stop words example\n",
    "for sentence in vietnamese_stopword[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency - Invert document frequency (TF-IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF\n",
    "Term frequency (TF) is the number of times a given term appears in document\n",
    "\n",
    "$$\n",
    "tf(t) = f(t,d)\\times\\frac{1}{T}\n",
    "$$\n",
    "whereas, $f(t,d)$ is the frequency of the word $t$ in the document $d$, $T$ is the number of all words in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf\n",
       "and       0.0\n",
       "document  0.2\n",
       "first     0.2\n",
       "is        0.2\n",
       "one       0.0\n",
       "second    0.0\n",
       "the       0.2\n",
       "third     0.0\n",
       "this      0.2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Declare TF vectorize\n",
    "tf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                use_idf=False, # only using TF\n",
    "                                norm='l1')\n",
    "\n",
    "tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf_vectorized = tf_vectorizer.transform(corpus)\n",
    "\n",
    "tf_output = tf_vectorized[0]\n",
    "\n",
    "# Build TF table\n",
    "words_tf_idf = pd.DataFrame(tf_output.T.todense(), index=tf_vectorizer.get_feature_names_out(), columns=['tf'])\n",
    "words_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF\n",
    "\n",
    "Inverse Document Frequency, or abbreviated as IDF, measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones.\n",
    "\n",
    "$$\n",
    "idf(t) = \\log\\left(\\frac{\\text{#documents in the document set}}{\\text{#documents with term}}\\right) + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf       idf\n",
       "is        0.2  1.000000\n",
       "the       0.2  1.000000\n",
       "this      0.2  1.000000\n",
       "document  0.2  1.287682\n",
       "first     0.2  1.693147\n",
       "and       0.0  2.386294\n",
       "second    0.0  2.386294\n",
       "one       0.0  2.386294\n",
       "third     0.0  2.386294"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Configure settings for IDF vectorize\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                    smooth_idf=False,\n",
    "                                    use_idf=True,\n",
    "                                    norm=None)\n",
    "\n",
    "tf_idf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Retrieve only idf information\n",
    "idf_vectorizer = tf_idf_vectorizer.idf_\n",
    "\n",
    "# Join idf values into the previous dataframe\n",
    "words_tf_idf['idf'] = idf_vectorizer\n",
    "\n",
    "# Show dataframe with ascending values of idf\n",
    "words_tf_idf.sort_values(by=['idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Technically saying, TF-IDF is a score which is applied to every word in every document in our dataset. And for every word, the TF-IDF value increases with every appearance of the word in a document, but is gradually decreased with every appearance in other documents\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}= tf(t, d) \\times idf(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.215302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.283096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf       idf    tf-idf\n",
       "and       0.0  2.386294  0.000000\n",
       "third     0.0  2.386294  0.000000\n",
       "second    0.0  2.386294  0.000000\n",
       "one       0.0  2.386294  0.000000\n",
       "is        0.2  1.000000  0.167201\n",
       "the       0.2  1.000000  0.167201\n",
       "this      0.2  1.000000  0.167201\n",
       "document  0.2  1.287682  0.215302\n",
       "first     0.2  1.693147  0.283096"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                    smooth_idf=False,\n",
    "                                    use_idf=True,\n",
    "                                    norm='l1')\n",
    "\n",
    "tf_idf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf_idf_vectorized = tf_idf_vectorizer.transform(corpus)\n",
    "\n",
    "tf_idf_output = tf_idf_vectorized[0]\n",
    "words_tf_idf['tf-idf'] = tf_idf_output.T.todense()\n",
    "\n",
    "words_tf_idf.sort_values(by=['tf-idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem set 2\n",
    "Based on the problem 1 and the instruction on TF, IDF, TF-IDF:\n",
    "- (2a) Build the tf-idf table for the UIT-VSFC dataset with $n$-gram = 1 and $n$-gram = 2.\n",
    "- (2b) Change a few hyperparameters in the `TfidfVectorizer` function (`smooth_idf`, `sublinear_tf` and `norm`) from problem 2a (*you could browse from this [link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to discover which are the correct paramters to parse*). Explain the results differences collected after modifying hyperparameters.\n",
    "- (2c) Which words has the lowest and the highest tf-idf values ? Do they differ from $n$-grams results ?\n",
    "- (2d) Which limitations from $n$-grams that TF-IDF overcame ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 1-gram:\n",
      "           giáo     slide     trình       đầy        đủ\n",
      "0      0.447214  0.447214  0.447214  0.447214  0.447214\n",
      "1      0.447214  0.447214  0.447214  0.447214  0.447214\n",
      "2      0.447214  0.447214  0.447214  0.447214  0.447214\n",
      "3      0.447214  0.447214  0.447214  0.447214  0.447214\n",
      "4      0.447214  0.447214  0.447214  0.447214  0.447214\n",
      "...         ...       ...       ...       ...       ...\n",
      "11421  0.447214  0.447214  0.447214  0.447214  0.447214\n",
      "11422  0.447214  0.447214  0.447214  0.447214  0.447214\n",
      "11423  0.447214  0.447214  0.447214  0.447214  0.447214\n",
      "11424  0.447214  0.447214  0.447214  0.447214  0.447214\n",
      "11425  0.447214  0.447214  0.447214  0.447214  0.447214\n",
      "\n",
      "[11426 rows x 5 columns]\n",
      "\n",
      "TF-IDF with 2-gram:\n",
      "           giáo  giáo trình     slide  slide giáo     trình  trình đầy  \\\n",
      "0      0.333333    0.333333  0.333333    0.333333  0.333333   0.333333   \n",
      "1      0.333333    0.333333  0.333333    0.333333  0.333333   0.333333   \n",
      "2      0.333333    0.333333  0.333333    0.333333  0.333333   0.333333   \n",
      "3      0.333333    0.333333  0.333333    0.333333  0.333333   0.333333   \n",
      "4      0.333333    0.333333  0.333333    0.333333  0.333333   0.333333   \n",
      "...         ...         ...       ...         ...       ...        ...   \n",
      "11421  0.333333    0.333333  0.333333    0.333333  0.333333   0.333333   \n",
      "11422  0.333333    0.333333  0.333333    0.333333  0.333333   0.333333   \n",
      "11423  0.333333    0.333333  0.333333    0.333333  0.333333   0.333333   \n",
      "11424  0.333333    0.333333  0.333333    0.333333  0.333333   0.333333   \n",
      "11425  0.333333    0.333333  0.333333    0.333333  0.333333   0.333333   \n",
      "\n",
      "            đầy    đầy đủ        đủ  \n",
      "0      0.333333  0.333333  0.333333  \n",
      "1      0.333333  0.333333  0.333333  \n",
      "2      0.333333  0.333333  0.333333  \n",
      "3      0.333333  0.333333  0.333333  \n",
      "4      0.333333  0.333333  0.333333  \n",
      "...         ...       ...       ...  \n",
      "11421  0.333333  0.333333  0.333333  \n",
      "11422  0.333333  0.333333  0.333333  \n",
      "11423  0.333333  0.333333  0.333333  \n",
      "11424  0.333333  0.333333  0.333333  \n",
      "11425  0.333333  0.333333  0.333333  \n",
      "\n",
      "[11426 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# (2a) Build the tf-idf table for the UIT-VSFC dataset with $n$-gram = 1 and $n$-gram = 2\n",
    "tfidf_vectorizer_1gram = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf_vectorizer_2gram = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "tfidf_1gram = tfidf_vectorizer_1gram.fit_transform(list_all_sentence)\n",
    "tfidf_2gram = tfidf_vectorizer_2gram.fit_transform(list_all_sentence)\n",
    "\n",
    "tfidf_1gram_df = pd.DataFrame(tfidf_1gram.toarray(), columns=tfidf_vectorizer_1gram.get_feature_names_out())\n",
    "tfidf_2gram_df = pd.DataFrame(tfidf_2gram.toarray(), columns=tfidf_vectorizer_2gram.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF with 1-gram:\")\n",
    "print(tfidf_1gram_df)\n",
    "print(\"\\nTF-IDF with 2-gram:\")\n",
    "print(tfidf_2gram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 1-gram (sublinear_tf=True, smooth_idf=False, norm='l2'):\n",
      "       giáo  slide  trình  đầy   đủ\n",
      "0       0.2    0.2    0.2  0.2  0.2\n",
      "1       0.2    0.2    0.2  0.2  0.2\n",
      "2       0.2    0.2    0.2  0.2  0.2\n",
      "3       0.2    0.2    0.2  0.2  0.2\n",
      "4       0.2    0.2    0.2  0.2  0.2\n",
      "...     ...    ...    ...  ...  ...\n",
      "11421   0.2    0.2    0.2  0.2  0.2\n",
      "11422   0.2    0.2    0.2  0.2  0.2\n",
      "11423   0.2    0.2    0.2  0.2  0.2\n",
      "11424   0.2    0.2    0.2  0.2  0.2\n",
      "11425   0.2    0.2    0.2  0.2  0.2\n",
      "\n",
      "[11426 rows x 5 columns]\n",
      "\n",
      "TF-IDF with 2-gram (sublinear_tf=True, smooth_idf=False, norm='l2'):\n",
      "           giáo  giáo trình     slide  slide giáo     trình  trình đầy  \\\n",
      "0      0.111111    0.111111  0.111111    0.111111  0.111111   0.111111   \n",
      "1      0.111111    0.111111  0.111111    0.111111  0.111111   0.111111   \n",
      "2      0.111111    0.111111  0.111111    0.111111  0.111111   0.111111   \n",
      "3      0.111111    0.111111  0.111111    0.111111  0.111111   0.111111   \n",
      "4      0.111111    0.111111  0.111111    0.111111  0.111111   0.111111   \n",
      "...         ...         ...       ...         ...       ...        ...   \n",
      "11421  0.111111    0.111111  0.111111    0.111111  0.111111   0.111111   \n",
      "11422  0.111111    0.111111  0.111111    0.111111  0.111111   0.111111   \n",
      "11423  0.111111    0.111111  0.111111    0.111111  0.111111   0.111111   \n",
      "11424  0.111111    0.111111  0.111111    0.111111  0.111111   0.111111   \n",
      "11425  0.111111    0.111111  0.111111    0.111111  0.111111   0.111111   \n",
      "\n",
      "            đầy    đầy đủ        đủ  \n",
      "0      0.111111  0.111111  0.111111  \n",
      "1      0.111111  0.111111  0.111111  \n",
      "2      0.111111  0.111111  0.111111  \n",
      "3      0.111111  0.111111  0.111111  \n",
      "4      0.111111  0.111111  0.111111  \n",
      "...         ...       ...       ...  \n",
      "11421  0.111111  0.111111  0.111111  \n",
      "11422  0.111111  0.111111  0.111111  \n",
      "11423  0.111111  0.111111  0.111111  \n",
      "11424  0.111111  0.111111  0.111111  \n",
      "11425  0.111111  0.111111  0.111111  \n",
      "\n",
      "[11426 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# (2b) Change a few hyperparameters in the TfidfVectorizer function\n",
    "# 1-ngram\n",
    "tfidf_vectorizer_1gram = TfidfVectorizer(ngram_range=(1, 1), smooth_idf=False, sublinear_tf=True, norm='l1')\n",
    "tfidf_1gram = tfidf_vectorizer_1gram.fit_transform(list_all_sentence)\n",
    "tfidf_1gram_df = pd.DataFrame(tfidf_1gram.toarray(), columns=tfidf_vectorizer_1gram.get_feature_names_out())\n",
    "print(\"TF-IDF with 1-gram (sublinear_tf=True, smooth_idf=False, norm='l2'):\")\n",
    "print(tfidf_1gram_df)\n",
    "# 2-ngram\n",
    "tfidf_vectorizer_2gram = TfidfVectorizer(ngram_range=(1, 2), smooth_idf=False, sublinear_tf=True, norm='l1')\n",
    "tfidf_2gram = tfidf_vectorizer_2gram.fit_transform(list_all_sentence)\n",
    "tfidf_2gram_df = pd.DataFrame(tfidf_2gram.toarray(), columns=tfidf_vectorizer_2gram.get_feature_names_out())\n",
    "print(\"\\nTF-IDF with 2-gram (sublinear_tf=True, smooth_idf=False, norm='l2'):\")\n",
    "print(tfidf_2gram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest 1-gram TF-IDF values:\n",
      "giáo     0.2\n",
      "slide    0.2\n",
      "trình    0.2\n",
      "đầy      0.2\n",
      "đủ       0.2\n",
      "dtype: float64\n",
      "\n",
      "Highest 1-gram TF-IDF values:\n",
      "giáo     0.2\n",
      "slide    0.2\n",
      "trình    0.2\n",
      "đầy      0.2\n",
      "đủ       0.2\n",
      "dtype: float64\n",
      "\n",
      "Lowest 2-gram TF-IDF values:\n",
      "giáo          0.111111\n",
      "giáo trình    0.111111\n",
      "slide         0.111111\n",
      "slide giáo    0.111111\n",
      "trình         0.111111\n",
      "trình đầy     0.111111\n",
      "đầy           0.111111\n",
      "đầy đủ        0.111111\n",
      "đủ            0.111111\n",
      "dtype: float64\n",
      "\n",
      "Highest 2-gram TF-IDF values:\n",
      "giáo          0.111111\n",
      "giáo trình    0.111111\n",
      "slide         0.111111\n",
      "slide giáo    0.111111\n",
      "trình         0.111111\n",
      "trình đầy     0.111111\n",
      "đầy           0.111111\n",
      "đầy đủ        0.111111\n",
      "đủ            0.111111\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# (2c) Find words with the lowest and highest tf-idf values\n",
    "# 1-ngram\n",
    "lowest_1gram = tfidf_1gram_df.min(axis=0).sort_values(ascending=True).head(10)\n",
    "highest_1gram = tfidf_1gram_df.max(axis=0).sort_values(ascending=False).head(10)\n",
    "print(\"Lowest 1-gram TF-IDF values:\")\n",
    "print(lowest_1gram)\n",
    "print(\"\\nHighest 1-gram TF-IDF values:\")\n",
    "print(highest_1gram)\n",
    "# 2-ngram\n",
    "lowest_2gram = tfidf_2gram_df.min(axis=0).sort_values(ascending=True).head(10)\n",
    "highest_2gram = tfidf_2gram_df.max(axis=0).sort_values(ascending=False).head(10)\n",
    "print(\"\\nLowest 2-gram TF-IDF values:\")\n",
    "print(lowest_2gram)\n",
    "print(\"\\nHighest 2-gram TF-IDF values:\")\n",
    "print(highest_2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2d) Which limitations from $n$-grams that TF-IDF overcame ?\n",
    "# TF-IDF overcame the limitations of n\n",
    "# -grams by considering the importance of each word in the context of the entire document.\n",
    "# It assigns higher weights to words that are more informative and less frequent across the entire corpus, thus reducing the impact of common words that may not carry significant meaning.\n",
    "# TF-IDF also helps to mitigate the issue of high-dimensionality associated with n\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
